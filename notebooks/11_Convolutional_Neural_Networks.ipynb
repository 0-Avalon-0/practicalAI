{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Convolutional_Neural_Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bOChJSNXtC9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "metadata": {
        "id": "OLIxEDq6VhvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
        "\n",
        "In this lesson we will learn the basics of CNNs applied to image and text based data sources.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VoMq0eFRvugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ]
    },
    {
      "metadata": {
        "id": "ziGJNhiQeiGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn.png\" width=700>"
      ]
    },
    {
      "metadata": {
        "id": "qWro5T5qTJJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **Objective:**  Detect spatial substructure from input data to aid in classification, segmentation, etc.\n",
        "* **Advantages:** \n",
        "  * Small number of weights (shared)\n",
        "  * Parallelizable\n",
        "  * Detects spatial substrcutures (feature extractors)\n",
        "  * Interpretable via filters\n",
        "  * Used for in images/text/time-series etc.\n",
        "* **Disadvantages:**\n",
        "  * Many hyperparameters (kernel size, strides, etc.)\n",
        "  * Inputs have to be of same width (image dimensions, text length, etc.)\n",
        "* **Miscellaneous:** \n",
        "  * Lot's of deep CNN architectures constantly updated for SOTA performance"
      ]
    },
    {
      "metadata": {
        "id": "8nCsZGyWhI9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Filters"
      ]
    },
    {
      "metadata": {
        "id": "lxpgRzIjiVHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the core of CNNs are filters (weights, kernels, etc.) which convolve (slide) across our input to extract relevante features. The filters are initialized randomly but learn to pick up meaningful features from the input that aid in optimizing for the objective. We're going to teach CNNs in an unorthodox method where we entirely focus on applying it to 2D text data. Each input is composed of words and we will be representing each word as on-hot encoded vector which gives us our 2D input.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conv.gif\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "1kTABJyYj91S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading PyTorch library\n",
        "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kz9D2rrdmSl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFEbPKZFmSoZ",
        "colab_type": "code",
        "outputId": "467c80b7-1988-4891-815c-88204f023881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Assume all our inputs have the same # of words\n",
        "batch_size = 128\n",
        "sequence_size = 10 # words per input\n",
        "one_hot_size = 20 # vocab size\n",
        "x = torch.randn(batch_size, one_hot_size, sequence_size)\n",
        "print(\"Size: {}\".format(x.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 20, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8V4y9D75mSrA",
        "colab_type": "code",
        "outputId": "8eacd98b-704e-4e32-8a90-3a4dc0ac1b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Create filters for a conv layer\n",
        "out_channels = 96 # of filters\n",
        "kernel_size = 3 # filters are 3X3\n",
        "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=out_channels, kernel_size=kernel_size)\n",
        "print(\"Size: {}\".format(conv1.weight.shape))\n",
        "print(\"Filter size: {}\".format(conv1.out_channels))\n",
        "print(\"Filter size: {}\".format(conv1.kernel_size[0]))\n",
        "print(\"Padding: {}\".format(conv1.padding[0]))\n",
        "print(\"Stride: {}\".format(conv1.stride[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([96, 20, 3])\n",
            "Filter size: 96\n",
            "Filter size: 3\n",
            "Padding: 0\n",
            "Stride: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x40mC6Q3mStp",
        "colab_type": "code",
        "outputId": "9a768934-8003-413d-f78c-46d78a6f0eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Convolve using filters\n",
        "conv_output = conv1(x)\n",
        "print(\"Size: {}\".format(conv_output.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 96, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WE9ntwKOsZky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We get 128 for the batch size, 96 outputs because that's how many filters we used on the input, but where is the 8 coming from? You can visually apply the convolution or use this handy equation:\n",
        "\n",
        "$\\frac{W - F + 2P}{S} + 1 = \\frac{10 - 3 + 2(0)}{1} + 1 = 8$\n",
        "\n",
        "where:\n",
        "  * W: width of each input\n",
        "  * F: filter size\n",
        "  * P: padding\n",
        "  * S: stride"
      ]
    },
    {
      "metadata": {
        "id": "vwTtF7bBuZvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pooling"
      ]
    },
    {
      "metadata": {
        "id": "VXBbKPs1ua9G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The result of convolving filters on an input is a feature map. Due to the nature of convolution and overlaps, our feature map will have lots of redundant information. Pooling is a way to summarize a high-dimensional feature map into a lower dimensional one for simplified downstream computation. The pooling operation can be the max value, average, etc. in a certain receptive field.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/pool.jpeg\" width=450>"
      ]
    },
    {
      "metadata": {
        "id": "VCag6lk2mSwU",
        "colab_type": "code",
        "outputId": "25f928fc-bce9-4600-ff44-a770715867ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Max pooling\n",
        "kernel_size = 2\n",
        "pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
        "pool_output = pool1(conv_output)\n",
        "print(\"Size: {}\".format(pool_output.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 96, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c_e4QRFwvTt8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\frac{W-F}{S} + 1 = \\frac{8-2}{2} + 1 = 4$"
      ]
    },
    {
      "metadata": {
        "id": "l9rL1EWIfi-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNNs on text"
      ]
    },
    {
      "metadata": {
        "id": "aWtHDOJgHZvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're going use convolutional neural networks on text data which typically involves convolving on the character level representation of the text to capture meaningful n-grams. This could invovle: \n",
        "\n",
        "* 1D conv operations where inputs are words | $\\in \\mathbb{R}^{NXWXE}$\n",
        "    * where:\n",
        "    * N = batchsize\n",
        "    * W = max word length \n",
        "    * E = vocab_size (or embedding dim) at a char level\n",
        "    \n",
        "* 2D conv operations where inputs are sentences | $\\in \\mathbb{R}^{NXSXWXE}$\n",
        "    * where:\n",
        "    * N = batchsize\n",
        "    * S = max sentence length\n",
        "    * W = max word length \n",
        "    * E = vocab_size (or embedding dim) at a char level\n",
        "\n",
        "You can easily use this set up for [time series](https://arxiv.org/abs/1807.10707) data or [combine it](https://arxiv.org/abs/1808.04928) with other networks. For text data, we will create filters of varying kernel sizes (2, 3, 4) which act as feature selectors of varying n-gram sizes. The outputs are concated and fed into a fully-connected layer for class predictions. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn_text.png\" width=500>"
      ]
    },
    {
      "metadata": {
        "id": "bVBZxbaAtS9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ]
    },
    {
      "metadata": {
        "id": "y8QSdEcDtXUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "import collections\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VADCXjMwtXYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mpiCYECstXbT",
        "colab_type": "code",
        "outputId": "837bfc0e-83fd-42fc-cf50-c34e37f4c8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    data_file=\"names.csv\",\n",
        "    split_data_file=\"split_names.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"names\",\n",
        "    reload_from_files=False,\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    num_epochs=20,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    num_filters=100,\n",
        "    dropout_p=0.1,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "handle_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "print(\"Expanded filepaths: \")\n",
        "print(\"\\t{}\".format(args.vectorizer_file))\n",
        "print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded filepaths: \n",
            "\tnames/vectorizer.json\n",
            "\tnames/model.pth\n",
            "Using CUDA: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ptb4hJ4Bw8YU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "bNxZQUqfmS0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBdQpUTQtMgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Upload data from GitHub to notebook's local drive\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/surnames.csv\"\n",
        "response = urllib.request.urlopen(url)\n",
        "html = response.read()\n",
        "with open(args.data_file, 'wb') as fp:\n",
        "    fp.write(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PYCeGrStMj7",
        "colab_type": "code",
        "outputId": "52129d78-4a7a-4a13-ac9a-8f604d13e94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Raw data\n",
        "df = pd.read_csv(args.data_file, header=0)\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coté</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kore</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Koury</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lebzak</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality\n",
              "0  Woodford     English\n",
              "1      Coté      French\n",
              "2      Kore     English\n",
              "3     Koury      Arabic\n",
              "4    Lebzak     Russian"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "pbfVM-YatMnD",
        "colab_type": "code",
        "outputId": "9e069a27-5627-40f3-cb52-ddd3bb892e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "# Split by nationality\n",
        "by_nationality = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_nationality[row.nationality].append(row.to_dict())\n",
        "for nationality in by_nationality:\n",
        "    print (\"{0}: {1}\".format(nationality, len(by_nationality[nationality])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: 2972\n",
            "French: 229\n",
            "Arabic: 1603\n",
            "Russian: 2373\n",
            "Japanese: 775\n",
            "Chinese: 220\n",
            "Italian: 600\n",
            "Czech: 414\n",
            "Irish: 183\n",
            "German: 576\n",
            "Greek: 156\n",
            "Spanish: 258\n",
            "Polish: 120\n",
            "Dutch: 236\n",
            "Vietnamese: 58\n",
            "Korean: 77\n",
            "Portuguese: 55\n",
            "Scottish: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KdGOoKFjtMpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create split data\n",
        "final_list = []\n",
        "for _, item_list in sorted(by_nationality.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyDwlzzKtMsz",
        "colab_type": "code",
        "outputId": "a6e2052e-cb9a-4cef-b4c9-5d696558b8b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# df with split datasets\n",
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    7680\n",
              "test     1660\n",
              "val      1640\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "17aHMQOwtMvh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text\n",
        "    \n",
        "split_df.surname = split_df.surname.apply(preprocess_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wh6D8qfQmS2c",
        "colab_type": "code",
        "outputId": "9b9e21f7-4f2b-4ea6-f621-722fc7cf6870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "split_df.to_csv(args.split_data_file, index=False)\n",
        "split_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nationality</th>\n",
              "      <th>split</th>\n",
              "      <th>surname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>bishara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>nahas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>ghanem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>tannous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>mikhail</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  nationality  split  surname\n",
              "0      Arabic  train  bishara\n",
              "1      Arabic  train    nahas\n",
              "2      Arabic  train   ghanem\n",
              "3      Arabic  train  tannous\n",
              "4      Arabic  train  mikhail"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "6nZBgfQTuAA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "TeRVQlRZuBgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "        \n",
        "        # Add unknown token\n",
        "        self.add_unk = add_unk\n",
        "        self.unk_token = unk_token\n",
        "        if self.add_unk:\n",
        "            self.unk_index = self.add_token(self.unk_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx,\n",
        "                'add_unk': self.add_unk, 'unk_token': self.unk_token}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        if self.add_unk:\n",
        "            index = self.token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            index =  self.token_to_idx[token]\n",
        "        return index\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bH8LMH9wuBi9",
        "colab_type": "code",
        "outputId": "bfb485c8-b7a1-49ac-e4f1-1e6ae961a31d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Vocabulary instance\n",
        "nationality_vocab = Vocabulary(add_unk=False)\n",
        "for index, row in df.iterrows():\n",
        "    nationality_vocab.add_token(row.nationality)\n",
        "print (nationality_vocab) # __str__\n",
        "print (nationality_vocab.lookup_token(\"English\"))\n",
        "print (nationality_vocab.lookup_index(0))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=18)>\n",
            "0\n",
            "English\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "57a1lzHPuHHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "MwS5BEV-uBlt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
        "        self.surname_vocab = surname_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "        self.max_surname_length = max_surname_length\n",
        "\n",
        "    def vectorize(self, surname):\n",
        "        one_hot_matrix_size = (self.max_surname_length, len(self.surname_vocab))\n",
        "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
        "                               \n",
        "        for position_index, character in enumerate(surname):\n",
        "            character_index = self.surname_vocab.lookup_token(character)\n",
        "            one_hot_matrix[position_index][character_index] = 1\n",
        "        \n",
        "        return one_hot_matrix\n",
        "    \n",
        "    def unvectorize(self, one_hot_matrix):\n",
        "        len_name = int(np.sum(one_hot_matrix))\n",
        "        indices = np.zeros(len_name)\n",
        "        for i in range(len_name):\n",
        "            indices[i] = np.where(one_hot_matrix[i]==1)[0][0]\n",
        "        surname = [self.surname_vocab.lookup_index(index) for index in indices]\n",
        "        return surname\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df):\n",
        "        surname_vocab = Vocabulary(add_unk=True)\n",
        "        nationality_vocab = Vocabulary(add_unk=False)\n",
        "        max_surname_length = 0\n",
        "\n",
        "        # Create vocabularies\n",
        "        for index, row in df.iterrows():\n",
        "            max_surname_length = max(max_surname_length, len(row.surname))\n",
        "            for letter in row.surname: # char-level tokenization\n",
        "                surname_vocab.add_token(letter)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
        "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "        return cls(surname_vocab, nationality_vocab, \n",
        "                   max_surname_length=contents['max_surname_length'])\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable(),\n",
        "                'max_surname_length': self.max_surname_length}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zq7RoFAXuBo9",
        "colab_type": "code",
        "outputId": "d7769738-b1cb-4c2b-fc32-c33005c41103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorizer instance\n",
        "vectorizer = SurnameVectorizer.from_dataframe(split_df)\n",
        "print (vectorizer.surname_vocab)\n",
        "print (vectorizer.nationality_vocab)\n",
        "vectorized_surname = vectorizer.vectorize(preprocess_text(\"goku\"))\n",
        "print (np.shape(vectorized_surname))\n",
        "print (vectorized_surname)\n",
        "print (vectorizer.unvectorize(vectorized_surname))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=28)>\n",
            "<Vocabulary(size=18)>\n",
            "(17, 28)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n",
            "['g', 'o', 'k', 'u']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwD5PVkgZ-mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The inputs into a CNN must all have the same shape. Therefore, we determine the largest surname and make sure that all names meet that max length. For shorter names, we pad it with zeros to meet the max length. "
      ]
    },
    {
      "metadata": {
        "id": "wwQ8MNp5ZfeG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**: Unlike the bagged ont-hot encoding method in the MLP notebook, we are able to preserve the semantic structure of the surnames. We are able to use one-hot encoding here because we are using characters but when we process text with large vocabularies, this method simply can't scale. We'll explore embedding based methods in subsequent notebooks. "
      ]
    },
    {
      "metadata": {
        "id": "Mnf7gXgKuOgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "id": "YYqzM53fuBrf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjolk855uPrA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.nationality.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, split_data_file):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, SurnameVectorizer.from_dataframe(train_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(\n",
        "            self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
        "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "        return {'surname': surname_vector, 'nationality': nationality_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
        "                                shuffle=shuffle, drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvy-CJVSuPuS",
        "colab_type": "code",
        "outputId": "e041ea89-6a51-453e-dacc-6e01774a1c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Dataset instance\n",
        "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "print (dataset) # __str__\n",
        "print (np.shape(dataset[5]['surname'])) # __getitem__\n",
        "print (dataset.class_weights)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Dataset(split=train, size=7680)\n",
            "(17, 28)\n",
            "tensor([0.0006, 0.0045, 0.0024, 0.0042, 0.0003, 0.0044, 0.0017, 0.0064, 0.0055,\n",
            "        0.0017, 0.0013, 0.0130, 0.0083, 0.0182, 0.0004, 0.0133, 0.0039, 0.0172])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XY0CqM2Rd3Im",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "pWGpAzKPd32f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7Q0_nkjd30L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "       \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = [F.relu(conv(x)) for conv in self.conv]\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat(z, 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XlJwSKQkL_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "wLLmIuKRkNYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sV-Dc_5ykNgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
        "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print (\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
        "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            # Iterate over train dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, \n",
        "                device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # the training routine is these 5 steps:\n",
        "\n",
        "                # --------------------------------------\n",
        "                # step 1. zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # step 2. compute the output\n",
        "                y_pred = self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # step 4. use loss to produce gradients\n",
        "                loss.backward()\n",
        "\n",
        "                # step 5. use optimizer to take gradient step\n",
        "                self.optimizer.step()\n",
        "                # -----------------------------------------\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "\n",
        "            # Iterate over val dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "\n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "          \n",
        "    def run_test_loop(self):\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(\n",
        "            batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkeOQRwckNd1",
        "colab_type": "code",
        "outputId": "dffdadea-5e14-403f-b624-3bda35af3467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "if args.reload_from_files:\n",
        "    print (\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "        args.split_data_file,args.vectorizer_file)\n",
        "else:\n",
        "    print (\"Creating from scratch!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_channels=args.num_filters,\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating from scratch!\n",
            "<bound method Module.named_modules of SurnameModel(\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            "  (conv_bn): ModuleList(\n",
            "    (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1)\n",
            "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3JJdOO4ZkNb3",
        "colab_type": "code",
        "outputId": "872a1a70-19a3-4ffe-96d8-af80071a74de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "trainer.run_train_loop()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[EPOCH]: 0 | [LR]: 0.001 | [TRAIN LOSS]: 2.62 | [TRAIN ACC]: 23.9% | [VAL LOSS]: 2.27 | [VAL ACC]: 41.4%\n",
            "[EPOCH]: 1 | [LR]: 0.001 | [TRAIN LOSS]: 1.98 | [TRAIN ACC]: 44.7% | [VAL LOSS]: 1.90 | [VAL ACC]: 48.3%\n",
            "[EPOCH]: 2 | [LR]: 0.001 | [TRAIN LOSS]: 1.60 | [TRAIN ACC]: 51.8% | [VAL LOSS]: 1.65 | [VAL ACC]: 60.5%\n",
            "[EPOCH]: 3 | [LR]: 0.001 | [TRAIN LOSS]: 1.35 | [TRAIN ACC]: 57.3% | [VAL LOSS]: 1.55 | [VAL ACC]: 57.2%\n",
            "[EPOCH]: 4 | [LR]: 0.001 | [TRAIN LOSS]: 1.19 | [TRAIN ACC]: 60.0% | [VAL LOSS]: 1.47 | [VAL ACC]: 61.9%\n",
            "[EPOCH]: 5 | [LR]: 0.001 | [TRAIN LOSS]: 1.04 | [TRAIN ACC]: 63.8% | [VAL LOSS]: 1.38 | [VAL ACC]: 60.7%\n",
            "[EPOCH]: 6 | [LR]: 0.001 | [TRAIN LOSS]: 0.93 | [TRAIN ACC]: 66.6% | [VAL LOSS]: 1.34 | [VAL ACC]: 63.6%\n",
            "[EPOCH]: 7 | [LR]: 0.001 | [TRAIN LOSS]: 0.87 | [TRAIN ACC]: 67.6% | [VAL LOSS]: 1.37 | [VAL ACC]: 63.8%\n",
            "[EPOCH]: 8 | [LR]: 0.001 | [TRAIN LOSS]: 0.80 | [TRAIN ACC]: 70.5% | [VAL LOSS]: 1.29 | [VAL ACC]: 64.1%\n",
            "[EPOCH]: 9 | [LR]: 0.001 | [TRAIN LOSS]: 0.75 | [TRAIN ACC]: 70.2% | [VAL LOSS]: 1.31 | [VAL ACC]: 68.6%\n",
            "[EPOCH]: 10 | [LR]: 0.001 | [TRAIN LOSS]: 0.70 | [TRAIN ACC]: 72.2% | [VAL LOSS]: 1.27 | [VAL ACC]: 60.0%\n",
            "[EPOCH]: 11 | [LR]: 0.001 | [TRAIN LOSS]: 0.64 | [TRAIN ACC]: 72.6% | [VAL LOSS]: 1.30 | [VAL ACC]: 69.8%\n",
            "[EPOCH]: 12 | [LR]: 0.001 | [TRAIN LOSS]: 0.61 | [TRAIN ACC]: 74.3% | [VAL LOSS]: 1.32 | [VAL ACC]: 68.9%\n",
            "[EPOCH]: 13 | [LR]: 0.001 | [TRAIN LOSS]: 0.52 | [TRAIN ACC]: 77.4% | [VAL LOSS]: 1.29 | [VAL ACC]: 69.6%\n",
            "[EPOCH]: 14 | [LR]: 0.001 | [TRAIN LOSS]: 0.53 | [TRAIN ACC]: 76.7% | [VAL LOSS]: 1.32 | [VAL ACC]: 68.5%\n",
            "[EPOCH]: 15 | [LR]: 0.001 | [TRAIN LOSS]: 0.48 | [TRAIN ACC]: 78.0% | [VAL LOSS]: 1.30 | [VAL ACC]: 71.5%\n",
            "[EPOCH]: 16 | [LR]: 0.001 | [TRAIN LOSS]: 0.47 | [TRAIN ACC]: 78.7% | [VAL LOSS]: 1.34 | [VAL ACC]: 71.4%\n",
            "[EPOCH]: 17 | [LR]: 0.001 | [TRAIN LOSS]: 0.46 | [TRAIN ACC]: 79.2% | [VAL LOSS]: 1.34 | [VAL ACC]: 71.6%\n",
            "[EPOCH]: 18 | [LR]: 0.001 | [TRAIN LOSS]: 0.45 | [TRAIN ACC]: 79.1% | [VAL LOSS]: 1.32 | [VAL ACC]: 71.9%\n",
            "[EPOCH]: 19 | [LR]: 0.001 | [TRAIN LOSS]: 0.44 | [TRAIN ACC]: 79.5% | [VAL LOSS]: 1.30 | [VAL ACC]: 71.6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0QLZfEyznVpT",
        "colab_type": "code",
        "outputId": "81f90b51-8ffb-4da8-b17a-454fb95a08ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAE+CAYAAAD4XjP+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiHuL/B2aiLf/VLypHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f080cfe5f28>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BWGzMSaBnYMb",
        "colab_type": "code",
        "outputId": "f26dab13-c556-47ed-c3b6-e167979505ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
        "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.65\n",
            "Test Accuracy: 73.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5672VEginYnY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save all results\n",
        "trainer.save_train_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HN1g2vP3nad_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ]
    },
    {
      "metadata": {
        "id": "Myr8QQjKnZ7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Inference(object):\n",
        "    def __init__(self, model, vectorizer):\n",
        "        self.model = model\n",
        "        self.vectorizer = vectorizer\n",
        "  \n",
        "    def predict_nationality(self, surname):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top nationality\n",
        "        y_prob, indices = y_pred.max(dim=1)\n",
        "        index = indices.item()\n",
        "\n",
        "        # Predicted nationality\n",
        "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "        probability = y_prob.item()\n",
        "        return {'nationality': nationality, 'probability': probability}\n",
        "  \n",
        "    def predict_top_k(self, surname, k):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top k nationalities\n",
        "        y_prob, indices = torch.topk(y_pred, k=k)\n",
        "        probabilities = y_prob.detach().numpy()[0]\n",
        "        indices = indices.detach().numpy()[0]\n",
        "\n",
        "        # Results\n",
        "        results = []\n",
        "        for probability, index in zip(probabilities, indices):\n",
        "            nationality = self.vectorizer.nationality_vocab.lookup_index(index)\n",
        "            results.append({'nationality': nationality, 'probability': probability})\n",
        "\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vV2SBrXpdllN",
        "colab_type": "code",
        "outputId": "6837bccd-ecc9-438c-8d97-fd251f3e49f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "print (\"Reloading!\")\n",
        "dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "    args.split_data_file, args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_channels=args.num_filters,\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "model.load_state_dict(torch.load(args.model_state_file))\n",
        "model = model.to(args.device)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reloading!\n",
            "<bound method Module.named_modules of SurnameModel(\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1)\n",
            "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TRc5KCZinaBh",
        "colab_type": "code",
        "outputId": "8e1ec59d-b159-42ed-d167-c1dddd67b956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "inference = Inference(model=model, vectorizer=vectorizer)\n",
        "surname = input(\"Enter a surname to classify: \")\n",
        "prediction = inference.predict_nationality(preprocess_text(surname))\n",
        "print(\"{} -> {} (p={:0.2f})\".format(surname, prediction['nationality'], \n",
        "                                    prediction['probability']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a surname to classify: Goku\n",
            "Goku -> Japanese (p=0.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5slsQKwnZ_H",
        "colab_type": "code",
        "outputId": "3e60f098-03a8-465d-8a94-6e6377f356b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Top-k inference\n",
        "top_k = inference.predict_top_k(preprocess_text(surname), k=3)\n",
        "for result in top_k:\n",
        "    print (\"{} -> {} (p={:0.2f})\".format(surname, result['nationality'], \n",
        "                                         result['probability']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Goku -> Japanese (p=0.98)\n",
            "Goku -> Russian (p=0.01)\n",
            "Goku -> Czech (p=0.00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HQSsKNRSxjRB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch normalization"
      ]
    },
    {
      "metadata": {
        "id": "r3EamVazx2hx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Even though we standardized our inputs to have zero mean and unit variance to aid with convergence, our inputs change during training as they go through the different layers and nonlinearities. This is known as internal covariate shirt and it slows down training and requires us to use smaller learning rates. The solution is [batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm) which makes normalization a part of the model's architecture. This allows us to use much higher learning rates and get better performance, faster.\n",
        "\n",
        "$ BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
        "\n",
        "where:\n",
        "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
        "* $ \\mu_{x}$ = mean of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
        "* $\\sigma^2_{x}$ = variance of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
        "* $epsilon$ = noise\n",
        "* $\\gamma$ = scale parameter (learned parameter)\n",
        "* $\\beta$ = shift parameter (learned parameter)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9koMITOdzfZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But what does it mean for our activations to have zero mean and unit variance before the nonlinearity operation. It doesn't mean that the entire activation matrix has this property but instead batchnorm is applied on the hidden (num_channels in our case) dimension. So each hidden's mean and variance is calculated using all samples across the batch. Also, batchnorm uses the calcualted mean and variance of the activations in the batch during training. However, during test, the sample size could be skewed so the model uses the saved population mean and variance from training. PyTorch's [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) class takes care of all of this for us automatically.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/batchnorm.png\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "RsWdAKVEHvyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model with batch normalization\n",
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.conv_bn = nn.ModuleList([nn.BatchNorm1d(num_channels) # define batchnorms\n",
        "                                      for i in range(3)])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "       \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = [F.relu(conv_bn(conv(x))) for conv, conv_bn in zip(self.conv, self.conv_bn)]\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat(z, 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBXzxtiaxmXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can train this model with batch normalization and you'll notice that the validation results improve by ~2-5%."
      ]
    },
    {
      "metadata": {
        "id": "w6WRq-O3d1ba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO"
      ]
    },
    {
      "metadata": {
        "id": "oEcbaRswd1d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* image classification example\n",
        "* segmentation\n",
        "* deep CNN architectures\n",
        "* small 3X3 filters\n",
        "* details on padding and stride (control receptive field, make every pixel the center of the filter, etc.)\n",
        "* network-in-network (1x1 conv)\n",
        "* residual connections / residual block\n",
        "* interpretability (which n-grams fire)"
      ]
    }
  ]
}