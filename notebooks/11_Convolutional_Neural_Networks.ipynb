{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Convolutional_Neural_Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bOChJSNXtC9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "metadata": {
        "id": "OLIxEDq6VhvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
        "\n",
        "In this lesson we will learn the basics of CNNs applied to image and text based data sources.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VoMq0eFRvugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ]
    },
    {
      "metadata": {
        "id": "ziGJNhiQeiGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn.png\" width=700>"
      ]
    },
    {
      "metadata": {
        "id": "qWro5T5qTJJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **Objective:**  Detect spatial substructure from input data to aid in classification, segmentation, etc.\n",
        "* **Advantages:** \n",
        "  * Small number of weights (shared)\n",
        "  * Parallelizable\n",
        "  * Detects spatial substrcutures (feature extractors)\n",
        "  * Interpretable via filters\n",
        "  * Used for in images/text/time-series etc.\n",
        "* **Disadvantages:**\n",
        "  * Many hyperparameters (kernel size, strides, etc.)\n",
        "  * Inputs have to be of same width (image dimensions, text length, etc.)\n",
        "* **Miscellaneous:** \n",
        "  * Lot's of deep CNN architectures constantly updated for SOTA performance"
      ]
    },
    {
      "metadata": {
        "id": "8nCsZGyWhI9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Filters"
      ]
    },
    {
      "metadata": {
        "id": "lxpgRzIjiVHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the core of CNNs are filters (weights, kernels, etc.) which convolve (slide) across our input to extract relevante features. The filters are initialized randomly but learn to pick up meaningful features from the input that aid in optimizing for the objective. We're going to teach CNNs in an unorthodox method where we entirely focus on applying it to 2D text data. Each input is composed of words and we will be representing each word as on-hot encoded vector which gives us our 2D input.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conv.gif\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "1kTABJyYj91S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading PyTorch library\n",
        "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kz9D2rrdmSl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFEbPKZFmSoZ",
        "colab_type": "code",
        "outputId": "467c80b7-1988-4891-815c-88204f023881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Assume all our inputs have the same # of words\n",
        "batch_size = 128\n",
        "sequence_size = 10 # words per input\n",
        "one_hot_size = 20 # vocab size\n",
        "x = torch.randn(batch_size, one_hot_size, sequence_size)\n",
        "print(\"Size: {}\".format(x.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 20, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8V4y9D75mSrA",
        "colab_type": "code",
        "outputId": "8eacd98b-704e-4e32-8a90-3a4dc0ac1b96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Create filters for a conv layer\n",
        "out_channels = 96 # of filters\n",
        "kernel_size = 3 # filters are 3X3\n",
        "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=out_channels, kernel_size=kernel_size)\n",
        "print(\"Size: {}\".format(conv1.weight.shape))\n",
        "print(\"Filter size: {}\".format(conv1.out_channels))\n",
        "print(\"Filter size: {}\".format(conv1.kernel_size[0]))\n",
        "print(\"Padding: {}\".format(conv1.padding[0]))\n",
        "print(\"Stride: {}\".format(conv1.stride[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([96, 20, 3])\n",
            "Filter size: 96\n",
            "Filter size: 3\n",
            "Padding: 0\n",
            "Stride: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x40mC6Q3mStp",
        "colab_type": "code",
        "outputId": "9a768934-8003-413d-f78c-46d78a6f0eae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Convolve using filters\n",
        "conv_output = conv1(x)\n",
        "print(\"Size: {}\".format(conv_output.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 96, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WE9ntwKOsZky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We get 128 for the batch size, 96 outputs because that's how many filters we used on the input, but where is the 8 coming from? You can visually apply the convolution or use this handy equation:\n",
        "\n",
        "$\\frac{W - F + 2P}{S} + 1 = \\frac{10 - 3 + 2(0)}{1} + 1 = 8$\n",
        "\n",
        "where:\n",
        "  * W: width of each input\n",
        "  * F: filter size\n",
        "  * P: padding\n",
        "  * S: stride"
      ]
    },
    {
      "metadata": {
        "id": "vwTtF7bBuZvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pooling"
      ]
    },
    {
      "metadata": {
        "id": "VXBbKPs1ua9G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The result of convolving filters on an input is a feature map. Due to the nature of convolution and overlaps, our feature map will have lots of redundant information. Pooling is a way to summarize a high-dimensional feature map into a lower dimensional one for simplified downstream computation. The pooling operation can be the max value, average, etc. in a certain receptive field.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/pool.jpeg\" width=450>"
      ]
    },
    {
      "metadata": {
        "id": "VCag6lk2mSwU",
        "colab_type": "code",
        "outputId": "25f928fc-bce9-4600-ff44-a770715867ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Max pooling\n",
        "kernel_size = 2\n",
        "pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
        "pool_output = pool1(conv_output)\n",
        "print(\"Size: {}\".format(pool_output.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 96, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c_e4QRFwvTt8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\frac{W-F}{S} + 1 = \\frac{8-2}{2} + 1 = 4$"
      ]
    },
    {
      "metadata": {
        "id": "l9rL1EWIfi-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNNs on text"
      ]
    },
    {
      "metadata": {
        "id": "aWtHDOJgHZvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're going use convolutional neural networks on text data which typically involves convolving on the character level representation of the text to capture meaningful n-grams. This could invovle: \n",
        "\n",
        "* 1D conv operations where inputs are letters in a word or words in a sentence | $\\in \\mathbb{R}^{NXWXE}$ and outputs are (N, num_output_channels)\n",
        "    * where:\n",
        "    * N = batchsize\n",
        "    * W = max word length \n",
        "    * E = vocab_size (or embedding dim) at a char level\n",
        "    \n",
        "* 2D conv operations where inputs are words in a sentence represented at the character level| $\\in \\mathbb{R}^{NXSXWXE}$ and outputs are embeddings for each word (based on convlutions applied at the character level.) **This is the more typical way CNNs are used on text.** We'll see this in actions in a subsequent notebook.\n",
        "    * where:\n",
        "    * N = batchsize\n",
        "    * S = max sentence length\n",
        "    * W = max word length \n",
        "    * E = vocab_size (or embedding dim) at a char level\n",
        "\n",
        "You can easily use this set up for [time series](https://arxiv.org/abs/1807.10707) data or [combine it](https://arxiv.org/abs/1808.04928) with other networks. For text data, we will create filters of varying kernel sizes (2, 3, 4) which act as feature selectors of varying n-gram sizes. The outputs are concated and fed into a fully-connected layer for class predictions. The diagram below from this [paper](https://arxiv.org/abs/1510.03820) shows how 1D conv is applied to the words in a sentence. In our example, we will be applying 1D convolutions on letter in a word. In the [embeddings notebook](https://colab.research.google.com/github/GokuMohandas/practicalAI/blob/master/notebooks/12_Embeddings.ipynb), we will apply 1D convolutions on words in a sentence.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn_text.png\" width=500>"
      ]
    },
    {
      "metadata": {
        "id": "bVBZxbaAtS9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ]
    },
    {
      "metadata": {
        "id": "y8QSdEcDtXUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "import collections\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VADCXjMwtXYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mpiCYECstXbT",
        "colab_type": "code",
        "outputId": "837bfc0e-83fd-42fc-cf50-c34e37f4c8ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    data_file=\"names.csv\",\n",
        "    split_data_file=\"split_names.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"names\",\n",
        "    reload_from_files=False,\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    num_epochs=20,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    num_filters=100,\n",
        "    dropout_p=0.1,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "handle_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "print(\"Expanded filepaths: \")\n",
        "print(\"\\t{}\".format(args.vectorizer_file))\n",
        "print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded filepaths: \n",
            "\tnames/vectorizer.json\n",
            "\tnames/model.pth\n",
            "Using CUDA: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ptb4hJ4Bw8YU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "bNxZQUqfmS0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBdQpUTQtMgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Upload data from GitHub to notebook's local drive\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/surnames.csv\"\n",
        "response = urllib.request.urlopen(url)\n",
        "html = response.read()\n",
        "with open(args.data_file, 'wb') as fp:\n",
        "    fp.write(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PYCeGrStMj7",
        "colab_type": "code",
        "outputId": "52129d78-4a7a-4a13-ac9a-8f604d13e94d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Raw data\n",
        "df = pd.read_csv(args.data_file, header=0)\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coté</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kore</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Koury</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lebzak</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality\n",
              "0  Woodford     English\n",
              "1      Coté      French\n",
              "2      Kore     English\n",
              "3     Koury      Arabic\n",
              "4    Lebzak     Russian"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "pbfVM-YatMnD",
        "colab_type": "code",
        "outputId": "9e069a27-5627-40f3-cb52-ddd3bb892e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "# Split by nationality\n",
        "by_nationality = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_nationality[row.nationality].append(row.to_dict())\n",
        "for nationality in by_nationality:\n",
        "    print (\"{0}: {1}\".format(nationality, len(by_nationality[nationality])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: 2972\n",
            "French: 229\n",
            "Arabic: 1603\n",
            "Russian: 2373\n",
            "Japanese: 775\n",
            "Chinese: 220\n",
            "Italian: 600\n",
            "Czech: 414\n",
            "Irish: 183\n",
            "German: 576\n",
            "Greek: 156\n",
            "Spanish: 258\n",
            "Polish: 120\n",
            "Dutch: 236\n",
            "Vietnamese: 58\n",
            "Korean: 77\n",
            "Portuguese: 55\n",
            "Scottish: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KdGOoKFjtMpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create split data\n",
        "final_list = []\n",
        "for _, item_list in sorted(by_nationality.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyDwlzzKtMsz",
        "colab_type": "code",
        "outputId": "a6e2052e-cb9a-4cef-b4c9-5d696558b8b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# df with split datasets\n",
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    7680\n",
              "test     1660\n",
              "val      1640\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "17aHMQOwtMvh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text\n",
        "    \n",
        "split_df.surname = split_df.surname.apply(preprocess_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wh6D8qfQmS2c",
        "colab_type": "code",
        "outputId": "9b9e21f7-4f2b-4ea6-f621-722fc7cf6870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "split_df.to_csv(args.split_data_file, index=False)\n",
        "split_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nationality</th>\n",
              "      <th>split</th>\n",
              "      <th>surname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>bishara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>nahas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>ghanem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>tannous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>mikhail</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  nationality  split  surname\n",
              "0      Arabic  train  bishara\n",
              "1      Arabic  train    nahas\n",
              "2      Arabic  train   ghanem\n",
              "3      Arabic  train  tannous\n",
              "4      Arabic  train  mikhail"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "6nZBgfQTuAA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "TeRVQlRZuBgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "        \n",
        "        # Add unknown token\n",
        "        self.add_unk = add_unk\n",
        "        self.unk_token = unk_token\n",
        "        if self.add_unk:\n",
        "            self.unk_index = self.add_token(self.unk_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx,\n",
        "                'add_unk': self.add_unk, 'unk_token': self.unk_token}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        if self.add_unk:\n",
        "            index = self.token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            index =  self.token_to_idx[token]\n",
        "        return index\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bH8LMH9wuBi9",
        "colab_type": "code",
        "outputId": "bfb485c8-b7a1-49ac-e4f1-1e6ae961a31d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Vocabulary instance\n",
        "nationality_vocab = Vocabulary(add_unk=False)\n",
        "for index, row in df.iterrows():\n",
        "    nationality_vocab.add_token(row.nationality)\n",
        "print (nationality_vocab) # __str__\n",
        "print (nationality_vocab.lookup_token(\"English\"))\n",
        "print (nationality_vocab.lookup_index(0))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=18)>\n",
            "0\n",
            "English\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "57a1lzHPuHHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "MwS5BEV-uBlt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
        "        self.surname_vocab = surname_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "        self.max_surname_length = max_surname_length\n",
        "\n",
        "    def vectorize(self, surname):\n",
        "        one_hot_matrix_size = (self.max_surname_length, len(self.surname_vocab))\n",
        "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
        "                               \n",
        "        for position_index, character in enumerate(surname):\n",
        "            character_index = self.surname_vocab.lookup_token(character)\n",
        "            one_hot_matrix[position_index][character_index] = 1\n",
        "        \n",
        "        return one_hot_matrix\n",
        "    \n",
        "    def unvectorize(self, one_hot_matrix):\n",
        "        len_name = int(np.sum(one_hot_matrix))\n",
        "        indices = np.zeros(len_name)\n",
        "        for i in range(len_name):\n",
        "            indices[i] = np.where(one_hot_matrix[i]==1)[0][0]\n",
        "        surname = [self.surname_vocab.lookup_index(index) for index in indices]\n",
        "        return surname\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df):\n",
        "        surname_vocab = Vocabulary(add_unk=True)\n",
        "        nationality_vocab = Vocabulary(add_unk=False)\n",
        "        max_surname_length = 0\n",
        "\n",
        "        # Create vocabularies\n",
        "        for index, row in df.iterrows():\n",
        "            max_surname_length = max(max_surname_length, len(row.surname))\n",
        "            for letter in row.surname: # char-level tokenization\n",
        "                surname_vocab.add_token(letter)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
        "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "        return cls(surname_vocab, nationality_vocab, \n",
        "                   max_surname_length=contents['max_surname_length'])\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable(),\n",
        "                'max_surname_length': self.max_surname_length}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zq7RoFAXuBo9",
        "colab_type": "code",
        "outputId": "d7769738-b1cb-4c2b-fc32-c33005c41103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorizer instance\n",
        "vectorizer = SurnameVectorizer.from_dataframe(split_df)\n",
        "print (vectorizer.surname_vocab)\n",
        "print (vectorizer.nationality_vocab)\n",
        "vectorized_surname = vectorizer.vectorize(preprocess_text(\"goku\"))\n",
        "print (np.shape(vectorized_surname))\n",
        "print (vectorized_surname)\n",
        "print (vectorizer.unvectorize(vectorized_surname))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=28)>\n",
            "<Vocabulary(size=18)>\n",
            "(17, 28)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n",
            "['g', 'o', 'k', 'u']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwD5PVkgZ-mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The inputs into a CNN must all have the same shape. Therefore, we determine the largest surname and make sure that all names meet that max length. For shorter names, we pad it with zeros to meet the max length. "
      ]
    },
    {
      "metadata": {
        "id": "wwQ8MNp5ZfeG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**: Unlike the bagged ont-hot encoding method in the MLP notebook, we are able to preserve the semantic structure of the surnames. We are able to use one-hot encoding here because we are using characters but when we process text with large vocabularies, this method simply can't scale. We'll explore embedding based methods in subsequent notebooks. "
      ]
    },
    {
      "metadata": {
        "id": "Mnf7gXgKuOgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "id": "YYqzM53fuBrf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjolk855uPrA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.nationality.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, split_data_file):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, SurnameVectorizer.from_dataframe(train_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(\n",
        "            self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
        "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "        return {'surname': surname_vector, 'nationality': nationality_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
        "                                shuffle=shuffle, drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvy-CJVSuPuS",
        "colab_type": "code",
        "outputId": "e041ea89-6a51-453e-dacc-6e01774a1c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Dataset instance\n",
        "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "print (dataset) # __str__\n",
        "print (np.shape(dataset[5]['surname'])) # __getitem__\n",
        "print (dataset.class_weights)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Dataset(split=train, size=7680)\n",
            "(17, 28)\n",
            "tensor([0.0006, 0.0045, 0.0024, 0.0042, 0.0003, 0.0044, 0.0017, 0.0064, 0.0055,\n",
            "        0.0017, 0.0013, 0.0130, 0.0083, 0.0182, 0.0004, 0.0133, 0.0039, 0.0172])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XY0CqM2Rd3Im",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "pWGpAzKPd32f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7Q0_nkjd30L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "       \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = [F.relu(conv(x)) for conv in self.conv]\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat(z, 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XlJwSKQkL_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "wLLmIuKRkNYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sV-Dc_5ykNgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
        "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print (\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
        "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            # Iterate over train dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, \n",
        "                device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # the training routine is these 5 steps:\n",
        "\n",
        "                # --------------------------------------\n",
        "                # step 1. zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # step 2. compute the output\n",
        "                y_pred = self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # step 4. use loss to produce gradients\n",
        "                loss.backward()\n",
        "\n",
        "                # step 5. use optimizer to take gradient step\n",
        "                self.optimizer.step()\n",
        "                # -----------------------------------------\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "\n",
        "            # Iterate over val dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "\n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "          \n",
        "    def run_test_loop(self):\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(\n",
        "            batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkeOQRwckNd1",
        "colab_type": "code",
        "outputId": "fe36ca51-432c-40a8-e11d-5714f8a6ea93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "if args.reload_from_files:\n",
        "    print (\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "        args.split_data_file,args.vectorizer_file)\n",
        "else:\n",
        "    print (\"Creating from scratch!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_output_channels=args.num_filters,\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating from scratch!\n",
            "<bound method Module.named_modules of SurnameModel(\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            "  (conv_bn): ModuleList(\n",
            "    (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1)\n",
            "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3JJdOO4ZkNb3",
        "colab_type": "code",
        "outputId": "9c7fb63a-ebef-4a35-cd4a-f6845a7d30f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "trainer.run_train_loop()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[EPOCH]: 0 | [LR]: 0.001 | [TRAIN LOSS]: 2.67 | [TRAIN ACC]: 23.9% | [VAL LOSS]: 2.34 | [VAL ACC]: 40.8%\n",
            "[EPOCH]: 1 | [LR]: 0.001 | [TRAIN LOSS]: 2.03 | [TRAIN ACC]: 44.2% | [VAL LOSS]: 1.93 | [VAL ACC]: 45.9%\n",
            "[EPOCH]: 2 | [LR]: 0.001 | [TRAIN LOSS]: 1.61 | [TRAIN ACC]: 53.2% | [VAL LOSS]: 1.68 | [VAL ACC]: 57.3%\n",
            "[EPOCH]: 3 | [LR]: 0.001 | [TRAIN LOSS]: 1.37 | [TRAIN ACC]: 57.2% | [VAL LOSS]: 1.51 | [VAL ACC]: 56.1%\n",
            "[EPOCH]: 4 | [LR]: 0.001 | [TRAIN LOSS]: 1.18 | [TRAIN ACC]: 60.8% | [VAL LOSS]: 1.45 | [VAL ACC]: 61.9%\n",
            "[EPOCH]: 5 | [LR]: 0.001 | [TRAIN LOSS]: 1.04 | [TRAIN ACC]: 64.3% | [VAL LOSS]: 1.38 | [VAL ACC]: 63.2%\n",
            "[EPOCH]: 6 | [LR]: 0.001 | [TRAIN LOSS]: 0.95 | [TRAIN ACC]: 65.4% | [VAL LOSS]: 1.30 | [VAL ACC]: 65.2%\n",
            "[EPOCH]: 7 | [LR]: 0.001 | [TRAIN LOSS]: 0.85 | [TRAIN ACC]: 68.4% | [VAL LOSS]: 1.32 | [VAL ACC]: 64.2%\n",
            "[EPOCH]: 8 | [LR]: 0.001 | [TRAIN LOSS]: 0.78 | [TRAIN ACC]: 70.0% | [VAL LOSS]: 1.28 | [VAL ACC]: 63.2%\n",
            "[EPOCH]: 9 | [LR]: 0.001 | [TRAIN LOSS]: 0.73 | [TRAIN ACC]: 70.9% | [VAL LOSS]: 1.31 | [VAL ACC]: 67.7%\n",
            "[EPOCH]: 10 | [LR]: 0.001 | [TRAIN LOSS]: 0.69 | [TRAIN ACC]: 71.3% | [VAL LOSS]: 1.22 | [VAL ACC]: 68.6%\n",
            "[EPOCH]: 11 | [LR]: 0.001 | [TRAIN LOSS]: 0.64 | [TRAIN ACC]: 73.5% | [VAL LOSS]: 1.26 | [VAL ACC]: 69.1%\n",
            "[EPOCH]: 12 | [LR]: 0.001 | [TRAIN LOSS]: 0.60 | [TRAIN ACC]: 74.8% | [VAL LOSS]: 1.25 | [VAL ACC]: 66.0%\n",
            "[EPOCH]: 13 | [LR]: 0.001 | [TRAIN LOSS]: 0.53 | [TRAIN ACC]: 76.1% | [VAL LOSS]: 1.30 | [VAL ACC]: 71.4%\n",
            "[EPOCH]: 14 | [LR]: 0.001 | [TRAIN LOSS]: 0.52 | [TRAIN ACC]: 76.7% | [VAL LOSS]: 1.29 | [VAL ACC]: 70.5%\n",
            "[EPOCH]: 15 | [LR]: 0.001 | [TRAIN LOSS]: 0.48 | [TRAIN ACC]: 78.2% | [VAL LOSS]: 1.27 | [VAL ACC]: 73.0%\n",
            "[EPOCH]: 16 | [LR]: 0.001 | [TRAIN LOSS]: 0.45 | [TRAIN ACC]: 78.5% | [VAL LOSS]: 1.29 | [VAL ACC]: 72.3%\n",
            "[EPOCH]: 17 | [LR]: 0.001 | [TRAIN LOSS]: 0.45 | [TRAIN ACC]: 79.0% | [VAL LOSS]: 1.27 | [VAL ACC]: 70.5%\n",
            "[EPOCH]: 18 | [LR]: 0.001 | [TRAIN LOSS]: 0.45 | [TRAIN ACC]: 79.2% | [VAL LOSS]: 1.25 | [VAL ACC]: 71.1%\n",
            "[EPOCH]: 19 | [LR]: 0.001 | [TRAIN LOSS]: 0.44 | [TRAIN ACC]: 79.3% | [VAL LOSS]: 1.30 | [VAL ACC]: 71.4%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0QLZfEyznVpT",
        "colab_type": "code",
        "outputId": "62f014e5-713e-43aa-8828-61d8b5a562aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAE+CAYAAAD4XjP+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf9W6vMcQYxFmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f080cc27978>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BWGzMSaBnYMb",
        "colab_type": "code",
        "outputId": "efb6577b-e983-49a1-ce2d-b5a879eea06f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
        "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.35\n",
            "Test Accuracy: 70.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5672VEginYnY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save all results\n",
        "trainer.save_train_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HN1g2vP3nad_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ]
    },
    {
      "metadata": {
        "id": "Myr8QQjKnZ7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Inference(object):\n",
        "    def __init__(self, model, vectorizer):\n",
        "        self.model = model\n",
        "        self.vectorizer = vectorizer\n",
        "  \n",
        "    def predict_nationality(self, surname):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top nationality\n",
        "        y_prob, indices = y_pred.max(dim=1)\n",
        "        index = indices.item()\n",
        "\n",
        "        # Predicted nationality\n",
        "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "        probability = y_prob.item()\n",
        "        return {'nationality': nationality, 'probability': probability}\n",
        "  \n",
        "    def predict_top_k(self, surname, k):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top k nationalities\n",
        "        y_prob, indices = torch.topk(y_pred, k=k)\n",
        "        probabilities = y_prob.detach().numpy()[0]\n",
        "        indices = indices.detach().numpy()[0]\n",
        "\n",
        "        # Results\n",
        "        results = []\n",
        "        for probability, index in zip(probabilities, indices):\n",
        "            nationality = self.vectorizer.nationality_vocab.lookup_index(index)\n",
        "            results.append({'nationality': nationality, 'probability': probability})\n",
        "\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vV2SBrXpdllN",
        "colab_type": "code",
        "outputId": "6837bccd-ecc9-438c-8d97-fd251f3e49f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "print (\"Reloading!\")\n",
        "dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "    args.split_data_file, args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_output_channels=args.num_filters,\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "model.load_state_dict(torch.load(args.model_state_file))\n",
        "model = model.to(args.device)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reloading!\n",
            "<bound method Module.named_modules of SurnameModel(\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1)\n",
            "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TRc5KCZinaBh",
        "colab_type": "code",
        "outputId": "8e1ec59d-b159-42ed-d167-c1dddd67b956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "inference = Inference(model=model, vectorizer=vectorizer)\n",
        "surname = input(\"Enter a surname to classify: \")\n",
        "prediction = inference.predict_nationality(preprocess_text(surname))\n",
        "print(\"{} -> {} (p={:0.2f})\".format(surname, prediction['nationality'], \n",
        "                                    prediction['probability']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a surname to classify: Goku\n",
            "Goku -> Japanese (p=0.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5slsQKwnZ_H",
        "colab_type": "code",
        "outputId": "3e60f098-03a8-465d-8a94-6e6377f356b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Top-k inference\n",
        "top_k = inference.predict_top_k(preprocess_text(surname), k=3)\n",
        "for result in top_k:\n",
        "    print (\"{} -> {} (p={:0.2f})\".format(surname, result['nationality'], \n",
        "                                         result['probability']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Goku -> Japanese (p=0.98)\n",
            "Goku -> Russian (p=0.01)\n",
            "Goku -> Czech (p=0.00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HQSsKNRSxjRB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch normalization"
      ]
    },
    {
      "metadata": {
        "id": "r3EamVazx2hx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Even though we standardized our inputs to have zero mean and unit variance to aid with convergence, our inputs change during training as they go through the different layers and nonlinearities. This is known as internal covariate shirt and it slows down training and requires us to use smaller learning rates. The solution is [batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm) which makes normalization a part of the model's architecture. This allows us to use much higher learning rates and get better performance, faster.\n",
        "\n",
        "$ BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
        "\n",
        "where:\n",
        "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
        "* $ \\mu_{x}$ = mean of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
        "* $\\sigma^2_{x}$ = variance of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
        "* $epsilon$ = noise\n",
        "* $\\gamma$ = scale parameter (learned parameter)\n",
        "* $\\beta$ = shift parameter (learned parameter)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9koMITOdzfZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But what does it mean for our activations to have zero mean and unit variance before the nonlinearity operation. It doesn't mean that the entire activation matrix has this property but instead batchnorm is applied on the hidden (num_output_channels in our case) dimension. So each hidden's mean and variance is calculated using all samples across the batch. Also, batchnorm uses the calcualted mean and variance of the activations in the batch during training. However, during test, the sample size could be skewed so the model uses the saved population mean and variance from training. PyTorch's [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) class takes care of all of this for us automatically.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/batchnorm.png\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "RsWdAKVEHvyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model with batch normalization\n",
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.conv_bn = nn.ModuleList([nn.BatchNorm1d(num_output_channels) # define batchnorms\n",
        "                                      for i in range(3)])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "       \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = [F.relu(conv_bn(conv(x))) for conv, conv_bn in zip(self.conv, self.conv_bn)]\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat(z, 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBXzxtiaxmXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can train this model with batch normalization and you'll notice that the validation results improve by ~2-5%."
      ]
    },
    {
      "metadata": {
        "id": "w6WRq-O3d1ba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO"
      ]
    },
    {
      "metadata": {
        "id": "oEcbaRswd1d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* image classification example\n",
        "* segmentation\n",
        "* deep CNN architectures\n",
        "* small 3X3 filters\n",
        "* details on padding and stride (control receptive field, make every pixel the center of the filter, etc.)\n",
        "* network-in-network (1x1 conv)\n",
        "* residual connections / residual block\n",
        "* interpretability (which n-grams fire)"
      ]
    }
  ]
}