{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Convolutional_Neural_Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bOChJSNXtC9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks"
      ]
    },
    {
      "metadata": {
        "id": "OLIxEDq6VhvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
        "\n",
        "In this lesson we will learn the basics of Convolutional Neural Networks (CNNs) applied to text for natural language processing (NLP) tasks. CNNs are traditionally used on images and there are plenty of [tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) that cover this. But we're going to focus on using CNN on text data which yields amazing results. \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VoMq0eFRvugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "The diagram below from this [paper](https://arxiv.org/abs/1510.03820) shows how 1D convolution is applied to the words in a sentence. "
      ]
    },
    {
      "metadata": {
        "id": "ziGJNhiQeiGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn_text.png\" width=500>"
      ]
    },
    {
      "metadata": {
        "id": "qWro5T5qTJJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **Objective:**  Detect spatial substructure from input data.\n",
        "* **Advantages:** \n",
        "  * Small number of weights (shared)\n",
        "  * Parallelizable\n",
        "  * Detects spatial substrcutures (feature extractors)\n",
        "  * Interpretable via filters\n",
        "  * Used for in images/text/time-series etc.\n",
        "* **Disadvantages:**\n",
        "  * Many hyperparameters (kernel size, strides, etc.)\n",
        "  * Inputs have to be of same width (image dimensions, text length, etc.)\n",
        "* **Miscellaneous:** \n",
        "  * Lot's of deep CNN architectures constantly updated for SOTA performance"
      ]
    },
    {
      "metadata": {
        "id": "8nCsZGyWhI9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Filters"
      ]
    },
    {
      "metadata": {
        "id": "lxpgRzIjiVHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the core of CNNs are filters (weights, kernels, etc.) which convolve (slide) across our input to extract relevante features. The filters are initialized randomly but learn to pick up meaningful features from the input that aid in optimizing for the objective. We're going to teach CNNs in an unorthodox method where we entirely focus on applying it to 2D text data. Each input is composed of words and we will be representing each word as one-hot encoded vector which gives us our 2D input. The intuition here is that each filter represents a feature and we will use this filter on other inputs to capture the same feature. This is known as parameter sharing.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conv.gif\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "1kTABJyYj91S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "850244a9-1f0c-4309-87a3-709541b60acd"
      },
      "cell_type": "code",
      "source": [
        "# Loading PyTorch library\n",
        "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (91.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 91.1MB 52.5MB/s \n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 13.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kz9D2rrdmSl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1q1FiiIHXjI_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our inputs are a batch of 2D text data. Let's make an input with 64 samples, where each sample has 8 words and each word is represented by a array of 10 values (one hot encoded with vocab size of 10). This gives our inputs the size (64, 8, 10). The [PyTorch CNN modules](https://pytorch.org/docs/stable/nn.html#convolution-functions) prefer inputs to have the channel dim (one hot vector dim in our case) to be in the second position, so our inputs are of shape (64, 10, 8)."
      ]
    },
    {
      "metadata": {
        "id": "tFfYwCcjZj79",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn_text1.png\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "b6G2nBvOxR-e",
        "colab_type": "code",
        "outputId": "ba807cb4-06a5-4023-9fba-bd25f8853572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Assume all our inputs have the same # of words\n",
        "batch_size = 64\n",
        "sequence_size = 8 # words per input\n",
        "one_hot_size = 10 # vocab size (num_input_channels)\n",
        "x = torch.randn(batch_size, one_hot_size, sequence_size)\n",
        "print(\"Size: {}\".format(x.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([64, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GJmtay_UZohM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We want to convolve on this input using filters. For simplicity we will use just 5 filters that is of size (1, 2) and has the same depth as the number of channels (one_hot_size). This gives our filter a shape of (5, 2, 10) but recall that PyTorch CNN modules prefer to have the channel dim (one hot vector dim in our case) to be in the second position so the filter is of shape (5, 10, 2)."
      ]
    },
    {
      "metadata": {
        "id": "ZJF0l88qb-21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn_text2.png\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "WMK2TzgDxR8B",
        "colab_type": "code",
        "outputId": "7c5e6d5c-0d72-4ef2-9194-07f6ff1f270e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Create filters for a conv layer\n",
        "out_channels = 5 # of filters\n",
        "kernel_size = 2 # filters size 2\n",
        "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=out_channels, kernel_size=kernel_size)\n",
        "print(\"Size: {}\".format(conv1.weight.shape))\n",
        "print(\"Filter size: {}\".format(conv1.kernel_size[0]))\n",
        "print(\"Padding: {}\".format(conv1.padding[0]))\n",
        "print(\"Stride: {}\".format(conv1.stride[0]))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([5, 10, 2])\n",
            "Filter size: 2\n",
            "Padding: 0\n",
            "Stride: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lAcYxhDIbeWE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When we apply this filter on our inputs, we receive an output of shape (64, 5, 7). We get 64 for the batch size, 5 for the channel dim because we used 5 filters and 7 for the conv outputs because:\n",
        "\n",
        "$\\frac{W - F + 2P}{S} + 1 = \\frac{8 - 2 + 2(0)}{1} + 1 = 7$\n",
        "\n",
        "where:\n",
        "  * W: width of each input\n",
        "  * F: filter size\n",
        "  * P: padding\n",
        "  * S: stride"
      ]
    },
    {
      "metadata": {
        "id": "2c_KKtP4hrJx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn_text3.png\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "yjxtrM89xR5a",
        "colab_type": "code",
        "outputId": "14ad5497-2b6a-4d21-9dca-6999b2ed1929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Convolve using filters\n",
        "conv_output = conv1(x)\n",
        "print(\"Size: {}\".format(conv_output.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([64, 5, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vwTtF7bBuZvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pooling"
      ]
    },
    {
      "metadata": {
        "id": "VXBbKPs1ua9G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The result of convolving filters on an input is a feature map. Due to the nature of convolution and overlaps, our feature map will have lots of redundant information. Pooling is a way to summarize a high-dimensional feature map into a lower dimensional one for simplified downstream computation. The pooling operation can be the max value, average, etc. in a certain receptive field.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/pool.jpeg\" width=450>"
      ]
    },
    {
      "metadata": {
        "id": "VCag6lk2mSwU",
        "colab_type": "code",
        "outputId": "8da59252-28a7-4b69-fa46-8e9b1d4fcff0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Max pooling\n",
        "kernel_size = 2\n",
        "pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
        "pool_output = pool1(conv_output)\n",
        "print(\"Size: {}\".format(pool_output.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([64, 5, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c_e4QRFwvTt8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\frac{W-F}{S} + 1 = \\frac{7-2}{2} + 1 =  \\text{floor }(2.5) + 1 = 3$"
      ]
    },
    {
      "metadata": {
        "id": "l9rL1EWIfi-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNNs on text"
      ]
    },
    {
      "metadata": {
        "id": "aWtHDOJgHZvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're going use convolutional neural networks on text data which typically involves convolving on the character level representation of the text to capture meaningful n-grams. \n",
        "\n",
        "You can easily use this set up for [time series](https://arxiv.org/abs/1807.10707) data or [combine it](https://arxiv.org/abs/1808.04928) with other networks. For text data, we will create filters of varying kernel sizes (1,2), (1,3), and (1,4) which act as feature selectors of varying n-gram sizes. The outputs are concated and fed into a fully-connected layer for class predictions. In our example, we will be applying 1D convolutions on letter in a word. In the [embeddings notebook](https://colab.research.google.com/github/GokuMohandas/practicalAI/blob/master/notebooks/12_Embeddings.ipynb), we will apply 1D convolutions on words in a sentence.\n",
        "\n",
        "**Word embeddings**: capture the temporal correlations among\n",
        "adjacent tokens so that similar words have similar representations. Ex. \"New Jersey\" is close to \"NJ\" is close to \"Garden State\", etc.\n",
        "\n",
        "**Char embeddings**: create representations that map words at a character level. Ex. \"toy\" and \"toys\" will be close to each other."
      ]
    },
    {
      "metadata": {
        "id": "bVBZxbaAtS9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ]
    },
    {
      "metadata": {
        "id": "y8QSdEcDtXUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "import collections\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VADCXjMwtXYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mpiCYECstXbT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c0c3b994-7ca9-4fa8-d8ae-db33d3e0f622"
      },
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    data_file=\"names.csv\",\n",
        "    split_data_file=\"split_names.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"names\",\n",
        "    reload_from_files=False,\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    num_epochs=20,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    num_filters=100,\n",
        "    dropout_p=0.1,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "handle_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "print(\"Expanded filepaths: \")\n",
        "print(\"\\t{}\".format(args.vectorizer_file))\n",
        "print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded filepaths: \n",
            "\tnames/vectorizer.json\n",
            "\tnames/model.pth\n",
            "Using CUDA: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ptb4hJ4Bw8YU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "bNxZQUqfmS0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBdQpUTQtMgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Upload data from GitHub to notebook's local drive\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/surnames.csv\"\n",
        "response = urllib.request.urlopen(url)\n",
        "html = response.read()\n",
        "with open(args.data_file, 'wb') as fp:\n",
        "    fp.write(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PYCeGrStMj7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c090a608-29f8-40e2-e7b1-3f1109e4e744"
      },
      "cell_type": "code",
      "source": [
        "# Raw data\n",
        "df = pd.read_csv(args.data_file, header=0)\n",
        "df.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coté</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kore</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Koury</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lebzak</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality\n",
              "0  Woodford     English\n",
              "1      Coté      French\n",
              "2      Kore     English\n",
              "3     Koury      Arabic\n",
              "4    Lebzak     Russian"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "pbfVM-YatMnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "35dcf028-892d-4bf2-c178-f0dac72535b8"
      },
      "cell_type": "code",
      "source": [
        "# Split by nationality\n",
        "by_nationality = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_nationality[row.nationality].append(row.to_dict())\n",
        "for nationality in by_nationality:\n",
        "    print (\"{0}: {1}\".format(nationality, len(by_nationality[nationality])))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: 2972\n",
            "French: 229\n",
            "Arabic: 1603\n",
            "Russian: 2373\n",
            "Japanese: 775\n",
            "Chinese: 220\n",
            "Italian: 600\n",
            "Czech: 414\n",
            "Irish: 183\n",
            "German: 576\n",
            "Greek: 156\n",
            "Spanish: 258\n",
            "Polish: 120\n",
            "Dutch: 236\n",
            "Vietnamese: 58\n",
            "Korean: 77\n",
            "Portuguese: 55\n",
            "Scottish: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KdGOoKFjtMpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create split data\n",
        "final_list = []\n",
        "for _, item_list in sorted(by_nationality.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyDwlzzKtMsz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bda4049b-50e6-4205-e3b1-05fb068c1df1"
      },
      "cell_type": "code",
      "source": [
        "# df with split datasets\n",
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    7680\n",
              "test     1660\n",
              "val      1640\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "17aHMQOwtMvh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text\n",
        "    \n",
        "split_df.surname = split_df.surname.apply(preprocess_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wh6D8qfQmS2c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3a6d5eee-e20e-4829-97d4-3ebf07ea645a"
      },
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "split_df.to_csv(args.split_data_file, index=False)\n",
        "split_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nationality</th>\n",
              "      <th>split</th>\n",
              "      <th>surname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>bishara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>nahas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>ghanem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>tannous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>mikhail</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  nationality  split  surname\n",
              "0      Arabic  train  bishara\n",
              "1      Arabic  train    nahas\n",
              "2      Arabic  train   ghanem\n",
              "3      Arabic  train  tannous\n",
              "4      Arabic  train  mikhail"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "6nZBgfQTuAA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "TeRVQlRZuBgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "        \n",
        "        # Add unknown token\n",
        "        self.add_unk = add_unk\n",
        "        self.unk_token = unk_token\n",
        "        if self.add_unk:\n",
        "            self.unk_index = self.add_token(self.unk_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx,\n",
        "                'add_unk': self.add_unk, 'unk_token': self.unk_token}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        if self.add_unk:\n",
        "            index = self.token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            index =  self.token_to_idx[token]\n",
        "        return index\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bH8LMH9wuBi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3fd0af55-b34b-4af4-f642-1cce43af1c14"
      },
      "cell_type": "code",
      "source": [
        "# Vocabulary instance\n",
        "nationality_vocab = Vocabulary(add_unk=False)\n",
        "for index, row in df.iterrows():\n",
        "    nationality_vocab.add_token(row.nationality)\n",
        "print (nationality_vocab) # __str__\n",
        "print (nationality_vocab.lookup_token(\"English\"))\n",
        "print (nationality_vocab.lookup_index(0))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=18)>\n",
            "0\n",
            "English\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "57a1lzHPuHHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "MwS5BEV-uBlt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
        "        self.surname_vocab = surname_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "        self.max_surname_length = max_surname_length\n",
        "\n",
        "    def vectorize(self, surname):\n",
        "        one_hot_matrix_size = (self.max_surname_length, len(self.surname_vocab))\n",
        "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
        "                               \n",
        "        for position_index, character in enumerate(surname):\n",
        "            character_index = self.surname_vocab.lookup_token(character)\n",
        "            one_hot_matrix[position_index][character_index] = 1\n",
        "        \n",
        "        return one_hot_matrix\n",
        "    \n",
        "    def unvectorize(self, one_hot_matrix):\n",
        "        len_name = int(np.sum(one_hot_matrix))\n",
        "        indices = np.zeros(len_name)\n",
        "        for i in range(len_name):\n",
        "            indices[i] = np.where(one_hot_matrix[i]==1)[0][0]\n",
        "        surname = [self.surname_vocab.lookup_index(index) for index in indices]\n",
        "        return surname\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df):\n",
        "        surname_vocab = Vocabulary(add_unk=True)\n",
        "        nationality_vocab = Vocabulary(add_unk=False)\n",
        "        max_surname_length = 0\n",
        "\n",
        "        # Create vocabularies\n",
        "        for index, row in df.iterrows():\n",
        "            max_surname_length = max(max_surname_length, len(row.surname))\n",
        "            for letter in row.surname: # char-level tokenization\n",
        "                surname_vocab.add_token(letter)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
        "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "        return cls(surname_vocab, nationality_vocab, \n",
        "                   max_surname_length=contents['max_surname_length'])\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable(),\n",
        "                'max_surname_length': self.max_surname_length}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zq7RoFAXuBo9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "03b1ab9b-fddc-44cf-dc94-41234f74cce0"
      },
      "cell_type": "code",
      "source": [
        "# Vectorizer instance\n",
        "vectorizer = SurnameVectorizer.from_dataframe(split_df)\n",
        "print (vectorizer.surname_vocab)\n",
        "print (vectorizer.nationality_vocab)\n",
        "vectorized_surname = vectorizer.vectorize(preprocess_text(\"goku\"))\n",
        "print (np.shape(vectorized_surname))\n",
        "print (vectorized_surname)\n",
        "print (vectorizer.unvectorize(vectorized_surname))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=28)>\n",
            "<Vocabulary(size=18)>\n",
            "(17, 28)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n",
            "['g', 'o', 'k', 'u']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwD5PVkgZ-mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The inputs into a CNN must all have the same shape. Therefore, we determine the largest surname and make sure that all names meet that max length. For shorter names, we pad it with zeros to meet the max length. "
      ]
    },
    {
      "metadata": {
        "id": "wwQ8MNp5ZfeG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**: Unlike the bagged ont-hot encoding method in the MLP notebook, we are able to preserve the semantic structure of the surnames. We are able to use one-hot encoding here because we are using characters but when we process text with large vocabularies, this method simply can't scale. We'll explore embedding based methods in subsequent notebooks. "
      ]
    },
    {
      "metadata": {
        "id": "Mnf7gXgKuOgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "id": "YYqzM53fuBrf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjolk855uPrA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.nationality.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, split_data_file):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, SurnameVectorizer.from_dataframe(train_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(\n",
        "            self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
        "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "        return {'surname': surname_vector, 'nationality': nationality_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
        "                                shuffle=shuffle, drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvy-CJVSuPuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9df1249f-17c3-4182-fdd6-ae697a0d66c8"
      },
      "cell_type": "code",
      "source": [
        "# Dataset instance\n",
        "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "print (dataset) # __str__\n",
        "print (np.shape(dataset[5]['surname'])) # __getitem__\n",
        "print (dataset.class_weights)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Dataset(split=train, size=7680)\n",
            "(17, 28)\n",
            "tensor([0.0006, 0.0045, 0.0024, 0.0042, 0.0003, 0.0044, 0.0017, 0.0064, 0.0055,\n",
            "        0.0017, 0.0013, 0.0130, 0.0083, 0.0182, 0.0004, 0.0133, 0.0039, 0.0172])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XY0CqM2Rd3Im",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "pWGpAzKPd32f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7Q0_nkjd30L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "       \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = [conv(x) for conv in self.conv]\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
        "        z = [F.relu(zz) for zz in z]\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat(z, 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XlJwSKQkL_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "wLLmIuKRkNYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sV-Dc_5ykNgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
        "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print (\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
        "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            # Iterate over train dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, \n",
        "                device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # the training routine is these 5 steps:\n",
        "\n",
        "                # --------------------------------------\n",
        "                # step 1. zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # step 2. compute the output\n",
        "                y_pred = self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # step 4. use loss to produce gradients\n",
        "                loss.backward()\n",
        "\n",
        "                # step 5. use optimizer to take gradient step\n",
        "                self.optimizer.step()\n",
        "                # -----------------------------------------\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "\n",
        "            # Iterate over val dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "\n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "          \n",
        "    def run_test_loop(self):\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(\n",
        "            batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkeOQRwckNd1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e239dc6f-043f-4247-8076-c602b8bfcabf"
      },
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "if args.reload_from_files:\n",
        "    print (\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "        args.split_data_file,args.vectorizer_file)\n",
        "else:\n",
        "    print (\"Creating from scratch!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_output_channels=args.num_filters,\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating from scratch!\n",
            "<bound method Module.named_modules of SurnameModel(\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1)\n",
            "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3JJdOO4ZkNb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5546b147-020b-4d84-b3b2-b3a2ac0e533e"
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "trainer.run_train_loop()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[EPOCH]: 0 | [LR]: 0.001 | [TRAIN LOSS]: 2.82 | [TRAIN ACC]: 21.5% | [VAL LOSS]: 2.71 | [VAL ACC]: 36.9%\n",
            "[EPOCH]: 1 | [LR]: 0.001 | [TRAIN LOSS]: 2.54 | [TRAIN ACC]: 42.6% | [VAL LOSS]: 2.39 | [VAL ACC]: 45.6%\n",
            "[EPOCH]: 2 | [LR]: 0.001 | [TRAIN LOSS]: 2.18 | [TRAIN ACC]: 48.0% | [VAL LOSS]: 2.10 | [VAL ACC]: 45.4%\n",
            "[EPOCH]: 3 | [LR]: 0.001 | [TRAIN LOSS]: 1.88 | [TRAIN ACC]: 51.8% | [VAL LOSS]: 1.89 | [VAL ACC]: 49.9%\n",
            "[EPOCH]: 4 | [LR]: 0.001 | [TRAIN LOSS]: 1.67 | [TRAIN ACC]: 54.5% | [VAL LOSS]: 1.77 | [VAL ACC]: 55.4%\n",
            "[EPOCH]: 5 | [LR]: 0.001 | [TRAIN LOSS]: 1.50 | [TRAIN ACC]: 58.2% | [VAL LOSS]: 1.65 | [VAL ACC]: 57.4%\n",
            "[EPOCH]: 6 | [LR]: 0.001 | [TRAIN LOSS]: 1.38 | [TRAIN ACC]: 60.6% | [VAL LOSS]: 1.56 | [VAL ACC]: 58.4%\n",
            "[EPOCH]: 7 | [LR]: 0.001 | [TRAIN LOSS]: 1.28 | [TRAIN ACC]: 61.7% | [VAL LOSS]: 1.54 | [VAL ACC]: 61.1%\n",
            "[EPOCH]: 8 | [LR]: 0.001 | [TRAIN LOSS]: 1.19 | [TRAIN ACC]: 63.7% | [VAL LOSS]: 1.45 | [VAL ACC]: 61.9%\n",
            "[EPOCH]: 9 | [LR]: 0.001 | [TRAIN LOSS]: 1.11 | [TRAIN ACC]: 65.1% | [VAL LOSS]: 1.42 | [VAL ACC]: 62.6%\n",
            "[EPOCH]: 10 | [LR]: 0.001 | [TRAIN LOSS]: 1.04 | [TRAIN ACC]: 66.7% | [VAL LOSS]: 1.38 | [VAL ACC]: 63.3%\n",
            "[EPOCH]: 11 | [LR]: 0.001 | [TRAIN LOSS]: 0.98 | [TRAIN ACC]: 67.7% | [VAL LOSS]: 1.34 | [VAL ACC]: 66.3%\n",
            "[EPOCH]: 12 | [LR]: 0.001 | [TRAIN LOSS]: 0.92 | [TRAIN ACC]: 68.5% | [VAL LOSS]: 1.34 | [VAL ACC]: 63.3%\n",
            "[EPOCH]: 13 | [LR]: 0.001 | [TRAIN LOSS]: 0.89 | [TRAIN ACC]: 68.6% | [VAL LOSS]: 1.31 | [VAL ACC]: 64.3%\n",
            "[EPOCH]: 14 | [LR]: 0.001 | [TRAIN LOSS]: 0.84 | [TRAIN ACC]: 69.9% | [VAL LOSS]: 1.32 | [VAL ACC]: 66.9%\n",
            "[EPOCH]: 15 | [LR]: 0.001 | [TRAIN LOSS]: 0.80 | [TRAIN ACC]: 70.7% | [VAL LOSS]: 1.30 | [VAL ACC]: 67.7%\n",
            "[EPOCH]: 16 | [LR]: 0.001 | [TRAIN LOSS]: 0.77 | [TRAIN ACC]: 71.4% | [VAL LOSS]: 1.27 | [VAL ACC]: 68.1%\n",
            "[EPOCH]: 17 | [LR]: 0.001 | [TRAIN LOSS]: 0.73 | [TRAIN ACC]: 72.1% | [VAL LOSS]: 1.28 | [VAL ACC]: 69.5%\n",
            "[EPOCH]: 18 | [LR]: 0.001 | [TRAIN LOSS]: 0.70 | [TRAIN ACC]: 73.3% | [VAL LOSS]: 1.28 | [VAL ACC]: 70.2%\n",
            "[EPOCH]: 19 | [LR]: 0.001 | [TRAIN LOSS]: 0.66 | [TRAIN ACC]: 74.8% | [VAL LOSS]: 1.23 | [VAL ACC]: 68.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0QLZfEyznVpT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "927e69df-623d-430b-d1bd-47072ba55d15"
      },
      "cell_type": "code",
      "source": [
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAE+CAYAAAD4XjP+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f1d4e58f5c0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BWGzMSaBnYMb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "543c26d6-bb07-4730-fb3f-9a483bd2eb43"
      },
      "cell_type": "code",
      "source": [
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
        "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.27\n",
            "Test Accuracy: 66.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5672VEginYnY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save all results\n",
        "trainer.save_train_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HN1g2vP3nad_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ]
    },
    {
      "metadata": {
        "id": "Myr8QQjKnZ7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Inference(object):\n",
        "    def __init__(self, model, vectorizer):\n",
        "        self.model = model\n",
        "        self.vectorizer = vectorizer\n",
        "  \n",
        "    def predict_nationality(self, surname):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top nationality\n",
        "        y_prob, indices = y_pred.max(dim=1)\n",
        "        index = indices.item()\n",
        "\n",
        "        # Predicted nationality\n",
        "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "        probability = y_prob.item()\n",
        "        return {'nationality': nationality, 'probability': probability}\n",
        "  \n",
        "    def predict_top_k(self, surname, k):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top k nationalities\n",
        "        y_prob, indices = torch.topk(y_pred, k=k)\n",
        "        probabilities = y_prob.detach().numpy()[0]\n",
        "        indices = indices.detach().numpy()[0]\n",
        "\n",
        "        # Results\n",
        "        results = []\n",
        "        for probability, index in zip(probabilities, indices):\n",
        "            nationality = self.vectorizer.nationality_vocab.lookup_index(index)\n",
        "            results.append({'nationality': nationality, 'probability': probability})\n",
        "\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vV2SBrXpdllN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "print (\"Reloading!\")\n",
        "dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "    args.split_data_file, args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_output_channels=args.num_filters,\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "model.load_state_dict(torch.load(args.model_state_file))\n",
        "model = model.to(args.device)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TRc5KCZinaBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "inference = Inference(model=model, vectorizer=vectorizer)\n",
        "surname = input(\"Enter a surname to classify: \")\n",
        "prediction = inference.predict_nationality(preprocess_text(surname))\n",
        "print(\"{} → {} (p={:0.2f})\".format(surname, prediction['nationality'], \n",
        "                                    prediction['probability']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P5slsQKwnZ_H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Top-k inference\n",
        "top_k = inference.predict_top_k(preprocess_text(surname), k=3)\n",
        "for result in top_k:\n",
        "    print (\"{} → {} (p={:0.2f})\".format(surname, result['nationality'], \n",
        "                                         result['probability']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQSsKNRSxjRB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch normalization"
      ]
    },
    {
      "metadata": {
        "id": "r3EamVazx2hx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Even though we standardized our inputs to have zero mean and unit variance to aid with convergence, our inputs change during training as they go through the different layers and nonlinearities. This is known as internal covariate shirt and it slows down training and requires us to use smaller learning rates. The solution is [batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm) which makes normalization a part of the model's architecture. This allows us to use much higher learning rates and get better performance, faster.\n",
        "\n",
        "$ BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
        "\n",
        "where:\n",
        "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
        "* $ \\mu_{x}$ = mean of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
        "* $\\sigma^2_{x}$ = variance of each hidden | $\\in \\mathbb{R}^{1XH}$\n",
        "* $epsilon$ = noise\n",
        "* $\\gamma$ = scale parameter (learned parameter)\n",
        "* $\\beta$ = shift parameter (learned parameter)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9koMITOdzfZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But what does it mean for our activations to have zero mean and unit variance before the nonlinearity operation. It doesn't mean that the entire activation matrix has this property but instead batchnorm is applied on the hidden (num_output_channels in our case) dimension. So each hidden's mean and variance is calculated using all samples across the batch. Also, batchnorm uses the calcualted mean and variance of the activations in the batch during training. However, during test, the sample size could be skewed so the model uses the saved population mean and variance from training. PyTorch's [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) class takes care of all of this for us automatically.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/batchnorm.png\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "RsWdAKVEHvyV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Model with batch normalization\n",
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.conv_bn = nn.ModuleList([nn.BatchNorm1d(num_output_channels) # define batchnorms\n",
        "                                      for i in range(3)])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "       \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_input_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x = x.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z = [F.relu(conv_bn(conv(x))) for conv, conv_bn in zip(self.conv, self.conv_bn)]\n",
        "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat(z, 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBXzxtiaxmXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can train this model with batch normalization and you'll notice that the validation results improve by ~2-5%."
      ]
    },
    {
      "metadata": {
        "id": "w6WRq-O3d1ba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO"
      ]
    },
    {
      "metadata": {
        "id": "oEcbaRswd1d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* image classification example\n",
        "* segmentation\n",
        "* deep CNN architectures\n",
        "* small 3X3 filters\n",
        "* details on padding and stride (control receptive field, make every pixel the center of the filter, etc.)\n",
        "* network-in-network (1x1 conv)\n",
        "* residual connections / residual block\n",
        "* interpretability (which n-grams fire)"
      ]
    }
  ]
}