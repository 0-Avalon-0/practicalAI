{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11_Convolutional_Neural_Networks",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bOChJSNXtC9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "metadata": {
        "id": "OLIxEDq6VhvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
        "\n",
        "In this lesson we will learn the basics of CNNs applied to image and text based data sources.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VoMq0eFRvugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Overview"
      ]
    },
    {
      "metadata": {
        "id": "ziGJNhiQeiGN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn.png\" width=700>"
      ]
    },
    {
      "metadata": {
        "id": "qWro5T5qTJJL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* **Objective:**  Detect spatial substructure from input data to aid in classification, segmentation, etc.\n",
        "* **Advantages:** \n",
        "  * Small number of weights (shared)\n",
        "  * Parallelizable\n",
        "  * Detects spatial substrcutures (feature extractors)\n",
        "  * Interpretable via filters\n",
        "  * Used for in images/text/time-series etc.\n",
        "* **Disadvantages:**\n",
        "  * Many hyperparameters (kernel size, strides, etc.)\n",
        "  * Inputs have to be of same width (image dimensions, text length, etc.)\n",
        "* **Miscellaneous:** \n",
        "  * Lot's of deep CNN architectures constantly updated for SOTA performance"
      ]
    },
    {
      "metadata": {
        "id": "8nCsZGyWhI9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Filters"
      ]
    },
    {
      "metadata": {
        "id": "lxpgRzIjiVHv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At the core of CNNs are filters (weights, kernels, etc.) which convolve (slide) across our input to extract relevante features. The filters are initialized randomly but learn to pick up meaningful features from the input that aid in optimizing for the objective. We're going to teach CNNs in an unorthodox method where we entirely focus on applying it to 2D text data. Each input is composed of words and we will be representing each word as on-hot encoded vector which gives us our 2D input.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/conv.gif\" width=400>"
      ]
    },
    {
      "metadata": {
        "id": "1kTABJyYj91S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "8a7e5876-6109-487f-e44f-e6812bd4c9ab"
      },
      "cell_type": "code",
      "source": [
        "# Loading PyTorch library\n",
        "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (91.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 91.1MB 52.0MB/s \n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 11.8MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kz9D2rrdmSl9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFEbPKZFmSoZ",
        "colab_type": "code",
        "outputId": "d2ef58c6-c9bb-41a8-c6be-916a60357ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Assume all our inputs have the same # of words\n",
        "batch_size = 128\n",
        "sequence_size = 10 # words per input\n",
        "one_hot_size = 20 # vocab size\n",
        "x = torch.randn(batch_size, one_hot_size, sequence_size)\n",
        "print(\"Size: {}\".format(x.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 20, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8V4y9D75mSrA",
        "colab_type": "code",
        "outputId": "9b58e69a-6f2a-4f81-9600-7f2ddaf9b338",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Create filters for a conv layer\n",
        "out_channels = 96 # of filters\n",
        "kernel_size = 3 # filters are 3X3\n",
        "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=out_channels, kernel_size=kernel_size)\n",
        "print(\"Size: {}\".format(conv1.weight.shape))\n",
        "print(\"Filter size: {}\".format(conv1.out_channels))\n",
        "print(\"Filter size: {}\".format(conv1.kernel_size[0]))\n",
        "print(\"Padding: {}\".format(conv1.padding[0]))\n",
        "print(\"Stride: {}\".format(conv1.stride[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([96, 20, 3])\n",
            "Filter size: 96\n",
            "Filter size: 3\n",
            "Padding: 0\n",
            "Stride: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x40mC6Q3mStp",
        "colab_type": "code",
        "outputId": "f77ff817-b917-492f-b921-1cdaddec239e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Convolve using filters\n",
        "conv_output = conv1(x)\n",
        "print(\"Size: {}\".format(conv_output.shape))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 96, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WE9ntwKOsZky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We get 128 for the batch size, 96 outputs because that's how many filters we used on the input, but where is the 8 coming from? You can visually apply the convolution or use this handy equation:\n",
        "\n",
        "$\\frac{W - F + 2P}{S} + 1 = \\frac{10 - 3 + 2(0)}{1} + 1 = 8$\n",
        "\n",
        "where:\n",
        "  * W: width of each input\n",
        "  * F: filter size\n",
        "  * P: padding\n",
        "  * S: stride"
      ]
    },
    {
      "metadata": {
        "id": "vwTtF7bBuZvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pooling"
      ]
    },
    {
      "metadata": {
        "id": "VXBbKPs1ua9G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The result of convolving filters on an input is a feature map. Due to the nature of convolution and overlaps, our feature map will have lots of redundant information. Pooling is a way to summarize a high-dimensional feature map into a lower dimensional one for simplified downstream computation. The pooling operation can be the max value, average, etc. in a certain receptive field.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/pool.jpeg\" width=450>"
      ]
    },
    {
      "metadata": {
        "id": "VCag6lk2mSwU",
        "colab_type": "code",
        "outputId": "aa37c395-b8cb-452a-c561-d593f3f0d73b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Max pooling\n",
        "kernel_size = 2\n",
        "pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
        "pool_output = pool1(conv_output)\n",
        "print(\"Size: {}\".format(pool_output.shape))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size: torch.Size([128, 96, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c_e4QRFwvTt8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "$\\frac{W-F}{S} + 1 = \\frac{8-2}{2} + 1 = 4$"
      ]
    },
    {
      "metadata": {
        "id": "l9rL1EWIfi-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# CNNs on text"
      ]
    },
    {
      "metadata": {
        "id": "aWtHDOJgHZvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're going use convolutional neural networks on text data, which involves 1D conv (nn.Conv1d) operations or 2D conv operations (if working with sentences). You can easily use this set up for time series data or extend it to 2D conv operations (nn.Conv1d) for image data, or nn.Conv3d for video processing. For text data, we will create filters of varying kernel sizes (2, 3, 4) which act as feature selectors of varying n-gram sizes. The outputs are concated and fed into a fully-connected layer for class predictions. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/cnn_text.jpg\" width=650>"
      ]
    },
    {
      "metadata": {
        "id": "bVBZxbaAtS9u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ]
    },
    {
      "metadata": {
        "id": "y8QSdEcDtXUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "import collections\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VADCXjMwtXYN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mpiCYECstXbT",
        "colab_type": "code",
        "outputId": "1a839433-7ca6-4361-c1c2-a88118d53056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    data_file=\"names.csv\",\n",
        "    split_data_file=\"split_names.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"names\",\n",
        "    reload_from_files=False,\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    num_epochs=20,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    num_filters=100,\n",
        "    dropout_p=0.1,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "handle_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "print(\"Expanded filepaths: \")\n",
        "print(\"\\t{}\".format(args.vectorizer_file))\n",
        "print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded filepaths: \n",
            "\tnames/vectorizer.json\n",
            "\tnames/model.pth\n",
            "Using CUDA: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ptb4hJ4Bw8YU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data"
      ]
    },
    {
      "metadata": {
        "id": "bNxZQUqfmS0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MBdQpUTQtMgu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Upload data from GitHub to notebook's local drive\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/surnames.csv\"\n",
        "response = urllib.request.urlopen(url)\n",
        "html = response.read()\n",
        "with open(args.data_file, 'wb') as fp:\n",
        "    fp.write(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PYCeGrStMj7",
        "colab_type": "code",
        "outputId": "29fa5b79-4d63-4547-eaba-cca5a99355a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Raw data\n",
        "df = pd.read_csv(args.data_file, header=0)\n",
        "df.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>surname</th>\n",
              "      <th>nationality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Woodford</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Coté</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Kore</td>\n",
              "      <td>English</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Koury</td>\n",
              "      <td>Arabic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lebzak</td>\n",
              "      <td>Russian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    surname nationality\n",
              "0  Woodford     English\n",
              "1      Coté      French\n",
              "2      Kore     English\n",
              "3     Koury      Arabic\n",
              "4    Lebzak     Russian"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "pbfVM-YatMnD",
        "colab_type": "code",
        "outputId": "1e76cdd2-a586-45de-b008-747c2d3ab6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "# Split by nationality\n",
        "by_nationality = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_nationality[row.nationality].append(row.to_dict())\n",
        "for nationality in by_nationality:\n",
        "    print (\"{0}: {1}\".format(nationality, len(by_nationality[nationality])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English: 2972\n",
            "French: 229\n",
            "Arabic: 1603\n",
            "Russian: 2373\n",
            "Japanese: 775\n",
            "Chinese: 220\n",
            "Italian: 600\n",
            "Czech: 414\n",
            "Irish: 183\n",
            "German: 576\n",
            "Greek: 156\n",
            "Spanish: 258\n",
            "Polish: 120\n",
            "Dutch: 236\n",
            "Vietnamese: 58\n",
            "Korean: 77\n",
            "Portuguese: 55\n",
            "Scottish: 75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KdGOoKFjtMpz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create split data\n",
        "final_list = []\n",
        "for _, item_list in sorted(by_nationality.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DyDwlzzKtMsz",
        "colab_type": "code",
        "outputId": "266c0846-fe63-4c47-8854-b883501d60b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# df with split datasets\n",
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    7680\n",
              "test     1660\n",
              "val      1640\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "17aHMQOwtMvh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text\n",
        "    \n",
        "split_df.surname = split_df.surname.apply(preprocess_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wh6D8qfQmS2c",
        "colab_type": "code",
        "outputId": "4528406f-4a21-48cc-98e3-ca938fe72ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# Save to CSV\n",
        "split_df.to_csv(args.split_data_file, index=False)\n",
        "split_df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>nationality</th>\n",
              "      <th>split</th>\n",
              "      <th>surname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>bishara</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>nahas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>ghanem</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>tannous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Arabic</td>\n",
              "      <td>train</td>\n",
              "      <td>mikhail</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  nationality  split  surname\n",
              "0      Arabic  train  bishara\n",
              "1      Arabic  train    nahas\n",
              "2      Arabic  train   ghanem\n",
              "3      Arabic  train  tannous\n",
              "4      Arabic  train  mikhail"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "6nZBgfQTuAA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "TeRVQlRZuBgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "        \n",
        "        # Add unknown token\n",
        "        self.add_unk = add_unk\n",
        "        self.unk_token = unk_token\n",
        "        if self.add_unk:\n",
        "            self.unk_index = self.add_token(self.unk_token)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx,\n",
        "                'add_unk': self.add_unk, 'unk_token': self.unk_token}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        if self.add_unk:\n",
        "            index = self.token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            index =  self.token_to_idx[token]\n",
        "        return index\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bH8LMH9wuBi9",
        "colab_type": "code",
        "outputId": "79e707ee-d29f-48ec-bcd6-16dd3cb6764d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Vocabulary instance\n",
        "nationality_vocab = Vocabulary(add_unk=False)\n",
        "for index, row in df.iterrows():\n",
        "    nationality_vocab.add_token(row.nationality)\n",
        "print (nationality_vocab) # __str__\n",
        "print (nationality_vocab.lookup_token(\"English\"))\n",
        "print (nationality_vocab.lookup_index(0))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=18)>\n",
            "0\n",
            "English\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "57a1lzHPuHHm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "MwS5BEV-uBlt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameVectorizer(object):\n",
        "    def __init__(self, surname_vocab, nationality_vocab, max_surname_length):\n",
        "        self.surname_vocab = surname_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "        self.max_surname_length = max_surname_length\n",
        "\n",
        "    def vectorize(self, surname):\n",
        "        one_hot_matrix_size = (self.max_surname_length, len(self.surname_vocab))\n",
        "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
        "                               \n",
        "        for position_index, character in enumerate(surname):\n",
        "            character_index = self.surname_vocab.lookup_token(character)\n",
        "            one_hot_matrix[position_index][character_index] = 1\n",
        "        \n",
        "        return one_hot_matrix\n",
        "    \n",
        "    def unvectorize(self, one_hot_matrix):\n",
        "        len_name = int(np.sum(one_hot_matrix))\n",
        "        indices = np.zeros(len_name)\n",
        "        for i in range(len_name):\n",
        "            indices[i] = np.where(one_hot_matrix[i]==1)[0][0]\n",
        "        surname = [self.surname_vocab.lookup_index(index) for index in indices]\n",
        "        return surname\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df):\n",
        "        surname_vocab = Vocabulary(add_unk=True)\n",
        "        nationality_vocab = Vocabulary(add_unk=False)\n",
        "        max_surname_length = 0\n",
        "\n",
        "        # Create vocabularies\n",
        "        for index, row in df.iterrows():\n",
        "            max_surname_length = max(max_surname_length, len(row.surname))\n",
        "            for letter in row.surname: # char-level tokenization\n",
        "                surname_vocab.add_token(letter)\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "        return cls(surname_vocab, nationality_vocab, max_surname_length)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
        "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "        return cls(surname_vocab, nationality_vocab, \n",
        "                   max_surname_length=contents['max_surname_length'])\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable(),\n",
        "                'max_surname_length': self.max_surname_length}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zq7RoFAXuBo9",
        "colab_type": "code",
        "outputId": "62d4aac0-5fb2-4cea-cec9-76b15d88e3cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorizer instance\n",
        "vectorizer = SurnameVectorizer.from_dataframe(split_df)\n",
        "print (vectorizer.surname_vocab)\n",
        "print (vectorizer.nationality_vocab)\n",
        "vectorized_surname = vectorizer.vectorize(preprocess_text(\"goku\"))\n",
        "print (np.shape(vectorized_surname))\n",
        "print (vectorized_surname)\n",
        "print (vectorizer.unvectorize(vectorized_surname))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=28)>\n",
            "<Vocabulary(size=18)>\n",
            "(17, 28)\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0.]]\n",
            "['g', 'o', 'k', 'u']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mwD5PVkgZ-mt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The inputs into a CNN must all have the same shape. Therefore, we determine the largest surname and make sure that all names meet that max length. For shorter names, we pad it with zeros to meet the max length. "
      ]
    },
    {
      "metadata": {
        "id": "wwQ8MNp5ZfeG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**: Unlike the bagged ont-hot encoding method in the MLP notebook, we are able to preserve the semantic structure of the surnames. We are able to use one-hot encoding here because we are using characters but when we process text with large vocabularies, this method simply can't scale. We'll explore embedding based methods in subsequent notebooks. "
      ]
    },
    {
      "metadata": {
        "id": "Mnf7gXgKuOgp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ]
    },
    {
      "metadata": {
        "id": "YYqzM53fuBrf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gjolk855uPrA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.nationality.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, split_data_file):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, SurnameVectorizer.from_dataframe(train_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(\n",
        "            self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
        "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "        return {'surname': surname_vector, 'nationality': nationality_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
        "                                shuffle=shuffle, drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hvy-CJVSuPuS",
        "colab_type": "code",
        "outputId": "1175b39f-d0c3-4448-fe39-c08473938ed3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Dataset instance\n",
        "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "print (dataset) # __str__\n",
        "print (np.shape(dataset[5]['surname'])) # __getitem__\n",
        "print (dataset.class_weights)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Dataset(split=train, size=7680)\n",
            "(17, 28)\n",
            "tensor([0.0006, 0.0045, 0.0024, 0.0042, 0.0003, 0.0044, 0.0017, 0.0064, 0.0055,\n",
            "        0.0017, 0.0013, 0.0130, 0.0083, 0.0182, 0.0004, 0.0133, 0.0039, 0.0172])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XY0CqM2Rd3Im",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "pWGpAzKPd32f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7Q0_nkjd30L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "       \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x_in, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x_in = x_in.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z1 = self.conv[0](x_in)       \n",
        "        z1 = F.max_pool1d(z1, z1.size(2)).squeeze(2)\n",
        "        z2 = self.conv[1](x_in)\n",
        "        z2 = F.max_pool1d(z2, z2.size(2)).squeeze(2)\n",
        "        z3 = self.conv[2](x_in)\n",
        "        z3 = F.max_pool1d(z3, z3.size(2)).squeeze(2)        \n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat([z1, z2, z3], 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XlJwSKQkL_C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "wLLmIuKRkNYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sV-Dc_5ykNgS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
        "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print (\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
        "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            # Iterate over train dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, \n",
        "                device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # the training routine is these 5 steps:\n",
        "\n",
        "                # --------------------------------------\n",
        "                # step 1. zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # step 2. compute the output\n",
        "                y_pred = self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # step 4. use loss to produce gradients\n",
        "                loss.backward()\n",
        "\n",
        "                # step 5. use optimizer to take gradient step\n",
        "                self.optimizer.step()\n",
        "                # -----------------------------------------\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "\n",
        "            # Iterate over val dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "\n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "          \n",
        "    def run_test_loop(self):\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(\n",
        "            batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            y_pred =  self.model(batch_dict['surname'])\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OkeOQRwckNd1",
        "colab_type": "code",
        "outputId": "bbd16304-cd9d-4714-8272-4f32a7f70d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "if args.reload_from_files:\n",
        "    print (\"Reloading!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "        args.split_data_file,args.vectorizer_file)\n",
        "else:\n",
        "    print (\"Creating from scratch!\")\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_channels=args.num_filters,\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating from scratch!\n",
            "<bound method Module.named_modules of SurnameModel(\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
            "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
            "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1)\n",
            "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3JJdOO4ZkNb3",
        "colab_type": "code",
        "outputId": "c22a8db7-d051-4605-8b49-bafdb5dbd618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "trainer.run_train_loop()"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[EPOCH]: 0 | [LR]: 0.001 | [TRAIN LOSS]: 2.81 | [TRAIN ACC]: 26.0% | [VAL LOSS]: 2.69 | [VAL ACC]: 41.2%\n",
            "[EPOCH]: 1 | [LR]: 0.001 | [TRAIN LOSS]: 2.48 | [TRAIN ACC]: 44.2% | [VAL LOSS]: 2.31 | [VAL ACC]: 48.5%\n",
            "[EPOCH]: 2 | [LR]: 0.001 | [TRAIN LOSS]: 2.09 | [TRAIN ACC]: 49.5% | [VAL LOSS]: 2.02 | [VAL ACC]: 48.9%\n",
            "[EPOCH]: 3 | [LR]: 0.001 | [TRAIN LOSS]: 1.82 | [TRAIN ACC]: 53.2% | [VAL LOSS]: 1.83 | [VAL ACC]: 52.3%\n",
            "[EPOCH]: 4 | [LR]: 0.001 | [TRAIN LOSS]: 1.60 | [TRAIN ACC]: 57.3% | [VAL LOSS]: 1.66 | [VAL ACC]: 57.7%\n",
            "[EPOCH]: 5 | [LR]: 0.001 | [TRAIN LOSS]: 1.45 | [TRAIN ACC]: 59.4% | [VAL LOSS]: 1.60 | [VAL ACC]: 60.9%\n",
            "[EPOCH]: 6 | [LR]: 0.001 | [TRAIN LOSS]: 1.31 | [TRAIN ACC]: 62.1% | [VAL LOSS]: 1.53 | [VAL ACC]: 59.0%\n",
            "[EPOCH]: 7 | [LR]: 0.001 | [TRAIN LOSS]: 1.21 | [TRAIN ACC]: 63.8% | [VAL LOSS]: 1.45 | [VAL ACC]: 61.7%\n",
            "[EPOCH]: 8 | [LR]: 0.001 | [TRAIN LOSS]: 1.11 | [TRAIN ACC]: 65.9% | [VAL LOSS]: 1.41 | [VAL ACC]: 60.2%\n",
            "[EPOCH]: 9 | [LR]: 0.001 | [TRAIN LOSS]: 1.05 | [TRAIN ACC]: 66.3% | [VAL LOSS]: 1.37 | [VAL ACC]: 62.5%\n",
            "[EPOCH]: 10 | [LR]: 0.001 | [TRAIN LOSS]: 0.97 | [TRAIN ACC]: 67.6% | [VAL LOSS]: 1.32 | [VAL ACC]: 63.3%\n",
            "[EPOCH]: 11 | [LR]: 0.001 | [TRAIN LOSS]: 0.92 | [TRAIN ACC]: 68.7% | [VAL LOSS]: 1.29 | [VAL ACC]: 64.9%\n",
            "[EPOCH]: 12 | [LR]: 0.001 | [TRAIN LOSS]: 0.86 | [TRAIN ACC]: 69.9% | [VAL LOSS]: 1.28 | [VAL ACC]: 65.5%\n",
            "[EPOCH]: 13 | [LR]: 0.001 | [TRAIN LOSS]: 0.82 | [TRAIN ACC]: 71.0% | [VAL LOSS]: 1.28 | [VAL ACC]: 66.1%\n",
            "[EPOCH]: 14 | [LR]: 0.001 | [TRAIN LOSS]: 0.77 | [TRAIN ACC]: 72.1% | [VAL LOSS]: 1.21 | [VAL ACC]: 65.7%\n",
            "[EPOCH]: 15 | [LR]: 0.001 | [TRAIN LOSS]: 0.73 | [TRAIN ACC]: 73.0% | [VAL LOSS]: 1.22 | [VAL ACC]: 67.5%\n",
            "[EPOCH]: 16 | [LR]: 0.001 | [TRAIN LOSS]: 0.69 | [TRAIN ACC]: 73.4% | [VAL LOSS]: 1.21 | [VAL ACC]: 67.5%\n",
            "[EPOCH]: 17 | [LR]: 0.001 | [TRAIN LOSS]: 0.65 | [TRAIN ACC]: 74.8% | [VAL LOSS]: 1.21 | [VAL ACC]: 67.8%\n",
            "[EPOCH]: 18 | [LR]: 0.001 | [TRAIN LOSS]: 0.63 | [TRAIN ACC]: 74.5% | [VAL LOSS]: 1.24 | [VAL ACC]: 69.7%\n",
            "[EPOCH]: 19 | [LR]: 0.001 | [TRAIN LOSS]: 0.62 | [TRAIN ACC]: 75.1% | [VAL LOSS]: 1.22 | [VAL ACC]: 70.8%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0QLZfEyznVpT",
        "colab_type": "code",
        "outputId": "1a1aa94b-69af-4df2-f94f-061c1c725a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "cell_type": "code",
      "source": [
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAE+CAYAAAD4XjP+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9b5a661cc0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "BWGzMSaBnYMb",
        "colab_type": "code",
        "outputId": "5332fdcf-c961-425c-c3dd-6c3541353e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
        "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.30\n",
            "Test Accuracy: 68.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5672VEginYnY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save all results\n",
        "trainer.save_train_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HN1g2vP3nad_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ]
    },
    {
      "metadata": {
        "id": "Myr8QQjKnZ7k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Inference(object):\n",
        "    def __init__(self, model, vectorizer):\n",
        "        self.model = model\n",
        "        self.vectorizer = vectorizer\n",
        "  \n",
        "    def predict_nationality(self, surname):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top nationality\n",
        "        y_prob, indices = y_pred.max(dim=1)\n",
        "        index = indices.item()\n",
        "\n",
        "        # Predicted nationality\n",
        "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "        probability = y_prob.item()\n",
        "        return {'nationality': nationality, 'probability': probability}\n",
        "  \n",
        "    def predict_top_k(self, surname, k):\n",
        "        # Forward pass\n",
        "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
        "        self.model.eval()\n",
        "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
        "\n",
        "        # Top k nationalities\n",
        "        y_prob, indices = torch.topk(y_pred, k=k)\n",
        "        probabilities = y_prob.detach().numpy()[0]\n",
        "        indices = indices.detach().numpy()[0]\n",
        "\n",
        "        # Results\n",
        "        results = []\n",
        "        for probability, index in zip(probabilities, indices):\n",
        "            nationality = self.vectorizer.nationality_vocab.lookup_index(index)\n",
        "            results.append({'nationality': nationality, 'probability': probability})\n",
        "\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vV2SBrXpdllN",
        "colab_type": "code",
        "outputId": "9e7f093e-30c8-42d9-bbbf-f4f5adbbdcc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "print (\"Reloading!\")\n",
        "dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
        "    args.split_data_file, args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
        "                     num_channels=args.num_filters\n",
        "                     num_classes=len(vectorizer.nationality_vocab),\n",
        "                     dropout_p=args.dropout_p)\n",
        "model.load_state_dict(torch.load(args.model_state_file))\n",
        "model = model.to(args.device)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-93-1402a9bf1224>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    num_classes=len(vectorizer.nationality_vocab),\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "TRc5KCZinaBh",
        "colab_type": "code",
        "outputId": "908d0c59-136c-4baa-c29c-1b96402e5429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "inference = Inference(model=model, vectorizer=vectorizer)\n",
        "surname = input(\"Enter a surname to classify: \")\n",
        "prediction = inference.predict_nationality(preprocess_text(surname))\n",
        "print(\"{} -> {} (p={:0.2f})\".format(surname, prediction['nationality'], \n",
        "                                    prediction['probability']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a surname to classify: Goku\n",
            "Goku -> Japanese (p=0.98)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P5slsQKwnZ_H",
        "colab_type": "code",
        "outputId": "b263fb67-a821-46ab-acdf-d534cb08be2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Top-k inference\n",
        "top_k = inference.predict_top_k(preprocess_text(surname), k=3)\n",
        "for result in top_k:\n",
        "    print (\"{} -> {} (p={:0.2f})\".format(surname, result['nationality'], \n",
        "                                         result['probability']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Goku -> Japanese (p=0.98)\n",
            "Goku -> Korean (p=0.01)\n",
            "Goku -> Czech (p=0.00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HQSsKNRSxjRB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Batch normalization"
      ]
    },
    {
      "metadata": {
        "id": "r3EamVazx2hx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Even though we standardized our inputs to have zero mean and unit variance to aid with convergence, our inputs change during training as they go through the different layers. This is known as internal covariate shirt and it slows down training and requires us to use smaller learning rates. The solution is [batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm) which makes normalization a part of the model's architecture. This allows us to use much higher learning rates and get better performance, faster.\n",
        "\n",
        "$ a = \\frac{x - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta $\n",
        "\n",
        "where:\n",
        "* $a$ = activation | $\\in \\mathbb{R}^{NXH}$ ($N$ is the number of samples, $H$ is the hidden dim)\n",
        "* $x$ = input | $\\in \\mathbb{R}^{NXH}$\n",
        "* $ \\mu_{x}$ = mean of input feature in x | $\\in \\mathbb{R}^{NXH}$\n",
        "* $\\sigma^2_{x}$ = variance of input feature in x | $\\in \\mathbb{R}^{NXH}$\n",
        "* $epsilon$ = noise\n",
        "* $\\gamma$ = scale parameter (learned parameter)\n",
        "* $\\beta$ = shift parameter (learned parameter)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9koMITOdzfZB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But what does it mean for our activations to have zero mean and unit variance. It doesn't mean that the entire activation matrix has this property but instead batchnorm is applied on the feature (num_channels in our case) dimension. So each feature's mean and variance is calculated using all samples across the batch. PyTorch's [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) function takes care of all of this for us automatically including the operations during inference.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/batchnorm.png\" width=500>"
      ]
    },
    {
      "metadata": {
        "id": "feUe-KGixjs-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SurnameModel(nn.Module):\n",
        "    def __init__(self, num_input_channels, num_channels, num_classes, dropout_p):\n",
        "        super(SurnameModel, self).__init__()\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_channels, \n",
        "                                             kernel_size=f) for f in [2,3,4]])\n",
        "        self.conv_bn = nn.ModuleList([nn.BatchNorm1d(num_channels) # define batchnorms\n",
        "                                      for i in range(3)])\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "     \n",
        "        # FC weights\n",
        "        self.fc1 = nn.Linear(num_channels*3, num_classes)\n",
        "\n",
        "    def forward(self, x_in, channel_first=False, apply_softmax=False):\n",
        "        \n",
        "        # Rearrange input so num_channels is in dim 1 (N, C, L)\n",
        "        if not channel_first:\n",
        "            x_in = x_in.transpose(1, 2)\n",
        "            \n",
        "        # Conv outputs\n",
        "        z1 = self.conv_bn[0](self.conv[0](x_in)) # apply batch norm   \n",
        "        z1 = F.max_pool1d(z1, z1.size(2)).squeeze(2)\n",
        "        z2 = self.conv_bn[0](self.conv[1](x_in))  # apply batch norm   \n",
        "        z2 = F.max_pool1d(z2, z2.size(2)).squeeze(2)\n",
        "        z3 = self.conv_bn[0](self.conv[2](x_in))  # apply batch norm   \n",
        "        z3 = F.max_pool1d(z3, z3.size(2)).squeeze(2)\n",
        "        \n",
        "        # Concat conv outputs\n",
        "        z = torch.cat([z1, z2, z3], 1)\n",
        "        z = self.dropout(z)\n",
        "\n",
        "        # FC layer\n",
        "        y_pred = self.fc1(z)\n",
        "        \n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBXzxtiaxmXi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can train this model with batch normalization and you'll notice that the validation results improve by ~5%."
      ]
    },
    {
      "metadata": {
        "id": "w6WRq-O3d1ba",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO"
      ]
    },
    {
      "metadata": {
        "id": "oEcbaRswd1d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* image classification example\n",
        "* segmentation\n",
        "* deep CNN architectures\n",
        "* small 3X3 filters\n",
        "* details on padding and stride (control receptive field, make every pixel the center of the filter, etc.)\n",
        "* network-in-network (1x1 conv)\n",
        "* residual connections / residual block\n",
        "* interpretability (which n-grams fire)"
      ]
    }
  ]
}